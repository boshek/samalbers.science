[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a data scientist living in Victoria, BC, Canada. I acknowledge and thank the Lkwungen People, also known as the Songhees, Esquimalt, and Beecher Bay First Nations communities, for allowing me to live, work and play on their lands. I spend most of my computer time in R. My goal with this blog is to mostly ramble. And perhaps learn a little about the web. Or some random thing that I am interested in. One day I hope to build a weather station. Then hopefully I‚Äôll blog about it. Let‚Äôs all look forward to that day.\nThe repository that builds this blog is located at https://github.com/boshek/samalbers.science"
  },
  {
    "objectID": "posts/one-small-step-for-a-package-one-giant-leap-for-a-task-view/index.html",
    "href": "posts/one-small-step-for-a-package-one-giant-leap-for-a-task-view/index.html",
    "title": "One small step for a package, one giant leap for a task view",
    "section": "",
    "text": "This is a bit of a grandiose title for a blog post. But it is important to have ambition right? In 2003, after releasing Michigan, Sufjan Stevens announced his intention to release one album as an ode to each of the fifty American states, appropriately named the Fifty States Project. Despite only completing two albums (the other being Illinois) I appreciate his ambition, and desire to be thorough (and also those wacky costumes).\nIt is that thoroughness which resonates with me and what I‚Äôd like to apply to the Hydrology CRAN Task View. Myself, along with Sam Zipper and Ilaria Prosdocimi, maintain the Hydrology task view with the express goal of creating a comprehensive and high‚Äìquality list of R packages related to the field of Hydrology. We introduced the task view, at a very high level, over at the rOpenSci blog last year. This post is the first in a series where I try some aspect of every package included in the task view. If Stevens‚Äô can declared his Fifty States Project why can‚Äôt I take on the Hydrology Task View Project? My first foray into this is not terribly ambitious as I am going to discuss one of my own packages, rsoi, which was recently updated on CRAN with a new version."
  },
  {
    "objectID": "posts/one-small-step-for-a-package-one-giant-leap-for-a-task-view/index.html#rsoi",
    "href": "posts/one-small-step-for-a-package-one-giant-leap-for-a-task-view/index.html#rsoi",
    "title": "One small step for a package, one giant leap for a task view",
    "section": "rsoi",
    "text": "rsoi\nrsoi started out as a package to acquire El Ni√±o-Southern Oscillation (ENSO) data and import it into R. This is the data that climate scientists use to determine which phase of the oscillation we are currently experiencing. Since that initial version, rsoi has gained several datasets and now functions more generally as simple R interface to as many climate indices as we can collect. All the raw data that rsoi accesses is directly from the US government‚Äôs National Oceanic and Atmospehric Administration (NOAA). Packages like rsoi facilitate reproducible workflows by providing convenient functions to access data stored on the web. (Sidebar: the most comprehensive package for accessing all manner of NOAA data is the rnoaa package by Scott Chamberlain which is also in the Hydrology task view üòâ). As of version 0.5.1 rsoi provides access to these data (with the corresponding rsoi function in parentheses):\n\nSouthern Oscillation Index (download_soi)\nOceanic Nino Index (download_oni)\nNorth Pacific Gyre Oscillation (download_npgo)\nNorth Atlantic Oscillation (download_nao)\nArctic Oscillation (download_ao)\nAntarctic Oscillation (download_aao)\nMultivariate ENSO Index Version 2 (download_mei)\n\nrsoi is on CRAN so installation proceeds typically:\n\ninstall.packages(\"rsoi\")\n\nThe usage of a rsoi revolves around the download_* functions. For example if you want to read in the Oceanic Nino Index data you can run this code:\n\nlibrary(rsoi)\n\noni &lt;- download_oni()\nhead(oni)\n\n# A tibble: 6 √ó 7\n   Year Month Date       dSST3.4   ONI ONI_month_window phase             \n  &lt;int&gt; &lt;ord&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;            &lt;fct&gt;             \n1  1950 Jan   1950-01-01   -1.62 NA    &lt;NA&gt;             &lt;NA&gt;              \n2  1950 Feb   1950-02-01   -1.32 -1.34 JFM              Cool Phase/La Nina\n3  1950 Mar   1950-03-01   -1.07 -1.17 FMA              Cool Phase/La Nina\n4  1950 Apr   1950-04-01   -1.11 -1.18 MAM              Cool Phase/La Nina\n5  1950 May   1950-05-01   -1.37 -1.07 AMJ              Cool Phase/La Nina\n6  1950 Jun   1950-06-01   -0.74 -0.85 MJJ              Cool Phase/La Nina\n\n\nEach climate index in rsoi is accessed by functions of the format download_[index abbreviation]. A quick plot of the ONI data demonstrates that we are currently in the Neutral ENSO phase (as of 19/01/2020).\n\n\n\n\n\n\n\n\n\nThis is the basic usage of rsoi that has been in place since it was first published to CRAN. Luckily Elio Campitelli found rsoi and made it better:\n\n\nMe: \"I've been downloading and reshaping ENSO data often. Maybe I should build an #rstats package to do it. \"Also me: \"Mh.. I wonder if someone else has already done it\"The internets: \"Here you go\"https://t.co/CC7hVFXEjY\n\n‚Äî Elio Campitelli (@d_olivaw) October 17, 2019\n\n\nAs of version 0.5.1, thanks to several contributions by Elio, rsoi now will optionally use a cache so that data are downloaded only once per R session. Not only this is more polite to NOAA as the data provider, this also provides some optimization for processes that repeateadly call rsoi functions. You can either use this cache in memory:\n\nlibrary(tictoc)\n\n## accessing from the web\ntic()\nsoi &lt;- download_soi(use_cache = TRUE)\ntoc()\n\n1.028 sec elapsed\n\n##same session, accessing from the memory cache\ntic()\nsoi &lt;- download_soi(use_cache = TRUE)\ntoc()\n\n0.006 sec elapsed\n\n\nOr you can save it locally:\n\nmei &lt;- download_mei(use_cache = TRUE, file = \"mei.csv\")\n\nSubsequent calls of download_mei that have the use_cache argument set to TRUE, will automatically import that data stored on disk rather than access it from the web. This works for each function in rsoi. This is a really nice contribution from Elio.\nA knock on CRAN Task Views, meritted or not, that I have heard is that they are simply an unvetted collection of packages. To a certain extent this is true as there is no formal process whereby a package gains acceptance to a task view. Rather package maintainers usually simply ask to be added and they are. This series of blog posts (n currently equalling 1) is designed to provide some additional exploration of packages in the Hydrology task view. Like Sufjan Stevens, this is an ambitious goal. However, writing a blog is much easier than making an album (have you heard Illinois?) so maybe I have a better chance."
  },
  {
    "objectID": "posts/you-schema/index.html",
    "href": "posts/you-schema/index.html",
    "title": "I Scream, You Scream, We All Scream for an Arrow Schema",
    "section": "",
    "text": "Always take the time to write a schema. Sometimes you have to. It is isn‚Äôt optional. But in less strongly typed languages like R and python, you can sometimes get away with not specifying a schema because there are such good tools for correctly inferring it. This post is an example of an instance where you absolutely do need to provide schema.\nA data structure that is popular is a hive partitioned dataset. This is a cheap data structure that is easy to create (just structured directories!), compatible with s3 and has many tools to take advantage of it. I say ‚Äúcheap‚Äù because it does not come with the overhead of a full fledged database but on some level can act like one. For example, you can efficiently query partitioned data with SQL just like a typical database.\nTo illustrate why schemas are important for a partitioned dataset we first need to create one. We are going to use R‚Äôs internal quakes data. The code below creates a reprex but the main thing to notice is that one partition, stations=50, contains an additional column called wave_height. This scenario mimics a situation where for that station, perhaps you gained a new sensor and thus collected more data (always good!).\n\nlibrary(arrow, warn.conflicts = FALSE)\nfor (i in unique(quakes$stations)) {\n  quakes_per_station &lt;- quakes[quakes$stations == i, ]\n  if (i == \"50\") {\n    quakes_per_station$wave_height &lt;- runif(nrow(quakes_per_station), 0, 100)\n  }\n  write_dataset(quakes_per_station, \"quakes_partitioned/\", partitioning = \"stations\")\n}\n\nThe directory quakes_partitioned will have a bunch of sub-directories of the formation {variable}={value}.\nquakes_partitioned/\n‚îú‚îÄ‚îÄ stations=10\nIf we examine just one of those files, we can see what it contains:\n\nread_parquet(\"quakes_partitioned/stations=10/part-0.parquet\")\n\n# A tibble: 20 √ó 4\n     lat  long depth   mag\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1 -21    182.   600   4.4\n 2 -23.6  181.   349   4  \n 3 -16.3  186     48   4.5\n 4 -20.1  184.   186   4.2\n 5 -15.0  182.   399   4.1\n 6 -19.1  169.   158   4.4\n 7 -17.7  185    383   4  \n 8 -21.0  181.   483   4.2\n 9 -27.2  182.    55   4.6\n10 -18.4  183.   343   4.1\n11 -20.3  182.   476   4.5\n12 -14.8  185.   294   4.1\n13 -17.6  182.   548   4.1\n14 -20.6  182.   518   4.2\n15 -25    180    488   4.5\n16 -17.8  185.   223   4.1\n17 -20.7  186.    80   4  \n18 -21.8  181    618   4.1\n19 -21.0  181.   616   4.3\n20 -17.7  188.    45   4.2\n\n\nNote that the station column has been dropped from the parquet file because it is contained within the partitioned structure (i.e.¬†the directory name).\nApache Arrow has considerable functionality for dealing with partitioned data via its dataset layer. The easiest way for us to take advantage of this in R is to use open_dataset, point it at the partitioned directory and tell it that there is a partitioned variable:\n\nopen_dataset(\"quakes_partitioned/\", partitioning = \"stations\")\n\nFileSystemDataset with 102 Parquet files\n5 columns\nlat: double\nlong: double\ndepth: int32\nmag: double\nstations: int32\n\nSee $metadata for additional Schema metadata\n\n\nThis is a very fast operation that quickly collects some basic metadata from our quakes dataset. However we can see clearly that we are missing the added water_temp column from station 50. If we investigate that single parquet file we can see that column there:\n\nread_parquet(\"quakes_partitioned/stations=50/part-0.parquet\")\n\n# A tibble: 10 √ó 5\n     lat  long depth   mag wave_height\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1 -22.6  181.   544   5         62.3 \n 2 -20.6  182.   529   5         83.3 \n 3 -22.9  173.    56   5.1       93.8 \n 4 -23.3  184    164   4.8       88.7 \n 5 -20.5  182.   559   4.9       88.8 \n 6 -26.5  178.   609   5          7.31\n 7 -25.0  180.   505   4.9       46.3 \n 8 -23.4  180.   541   4.6        5.21\n 9 -23.9  180.   524   4.6       12.9 \n10 -20.9  185.    82   4.9       47.0 \n\n\nSo what‚Äôs happened here? An arrow dataset just uses the first file to construct the metadata. Because water_temp is not present there, it is not picked up by a open_dataset call.\nHow can we fix this? A schema! If we take the time to create a schema, we can tell open_dataset: ‚Äúhey there is another column in there that we are also interested in‚Äù. The arrow R package provides a schema function to construct this:\n\nquake_schema &lt;- schema(\n  lat = float64(),\n  long = float64(),\n  depth = int32(),\n  mag = float64(),\n  wave_height = float64(),\n  station = string()\n)\nquake_schema\n\nSchema\nlat: double\nlong: double\ndepth: int32\nmag: double\nwave_height: double\nstation: string\n\n\nNow we can provide that schema to the open_dataset function and see what happens:\n\nquakes_dataset &lt;- open_dataset(\n  \"quakes_partitioned/\",\n  partitioning = \"stations\",\n  schema = quake_schema\n)\nquakes_dataset\n\nFileSystemDataset with 102 Parquet files\n6 columns\nlat: double\nlong: double\ndepth: int32\nmag: double\nwave_height: double\nstation: string\n\n\nopen_dataset has recognized our wave_height column! We explicitly told arrow about it through a schema and so now you can work with it as you would any other column.\nOne other option that is worth discussing is setting the unify_schemas argument to TRUE. From the arrow documentation:\n\nshould all data fragments (files, Datasets) be scanned in order to create a unified schema from them? If FALSE, only the first fragment will be inspected for its schema. Use this fast path when you know and trust that all fragments have an identical schema. The default is FALSE when creating a dataset from a directory path/URI or vector of file paths/URIs (because there may be many files and scanning may be slow) but TRUE when sources is a list of Datasets (because there should be few Datasets in the list and their Schemas are already in memory).\n\nThis is an excellent heuristic for when you should use unify_schemas. I‚Äôd also suggest that you also gain some benefit from explicitly writing our your schema types as well the fields being used. Also specifying the schema in the way discussed above happens to be a bit a tiny bit faster on these data:\n\nlibrary(bench)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nbench::mark(\n  set_schema = open_dataset(\n    \"quakes_partitioned/\",\n    partitioning = \"stations\",\n    schema = quake_schema\n  ) |&gt;\n    collect(),\n  unify_schema = open_dataset(\n    \"quakes_partitioned/\",\n    partitioning = \"stations\",\n    unify_schemas = TRUE\n  ) |&gt;\n    collect(),\n  check = FALSE\n)\n\n# A tibble: 2 √ó 6\n  expression        min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;   &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 set_schema     30.3ms   31.3ms      31.7    1.08MB     7.31\n2 unify_schema   35.6ms   36.2ms      27.3   16.38KB     4.97\n\n\nIt is also worth mentioning that this is not a R specific thing. The same pattern exists in pyarrow:\n\nimport pyarrow as pa\nimport pyarrow.dataset as ds\n\nquakes_partitioned = ds.dataset(\"quakes_partitioned/\", partitioning = \"hive\")\nquakes_partitioned.schema\n\nlat: double\nlong: double\ndepth: int32\nmag: double\nstations: int32\n-- schema metadata --\nr: 'A\n3\n263169\n197888\n5\nUTF-8\n531\n1\n531\n5\n254\n254\n254\n254\n254\n1026\n1\n2621' + 128\n\n\nI am not certain quite what the schema metadata is telling us here and that‚Äôll have to be an investigation for another day. But the thing you can notice here is that, again, in the absense of a schema, we don‚Äôt detect that wave_height column. But if we instead provide it, we are able to detect it:\n\nquake_schema = pa.schema(\n    [\n        (\"lat\", pa.float64()),\n        (\"long\", pa.float64()),\n        (\"depth\", pa.int32()),\n        (\"mag\", pa.float64()),\n        (\"wave_height\", pa.float64()),\n        (\"stations\", pa.string()),\n    ]\n)\n\nquakes_partitioned = ds.dataset(\"quakes_partitioned/\", partitioning = \"hive\", schema=quake_schema)\nquakes_partitioned.schema\n\nlat: double\nlong: double\ndepth: int32\nmag: double\nwave_height: double\nstations: string\n\n\nExplicitly setting your schema, illustrated here with Apache Arrow datasets, can be really important. You always have a schema - this process just reminds us that you either let it be inferred by another program or you tell your computer exactly what you want."
  },
  {
    "objectID": "posts/eccc-webservice/index.html",
    "href": "posts/eccc-webservice/index.html",
    "title": "The return of the web service",
    "section": "",
    "text": "The most common question I get about the tidyhydat package goes something like this:\nPreviously the answer was‚Ä¶ you can‚Äôt. The HYDAT database is a historical database of hydrometric data. Data are validated and entered into HYDAT periodically. It is not updated in realtime. At the same time realtime data is only available for 30 days from the datamart.\nNow, however, Environment and Climate Change Canada (ECCC) provided a web service that provides realtime data for stations which extends back to about 18 months. This usually spans the gap for current data to when it gets into HYDAT. And since tidyhydat version 0.6.0 you can now access this data in R via the realtime_ws function. This post is a quick introduction to some of the usage of the web service from tidyhydat.\nLet‚Äôs load a few packages to help illustrate this.\nlibrary(tidyhydat)\nlibrary(dplyr)\nlibrary(lubridate)"
  },
  {
    "objectID": "posts/eccc-webservice/index.html#using-the-web-service-for-realtime-hydrometric-data",
    "href": "posts/eccc-webservice/index.html#using-the-web-service-for-realtime-hydrometric-data",
    "title": "The return of the web service",
    "section": "Using the web service for realtime hydrometric data",
    "text": "Using the web service for realtime hydrometric data\nThe realtime_ws function operates in a similar way to most of the other functions in tidyhydat particularly the realtime_dd function. You can pass a single station or a vector of stations and the function returns a tibble of data relating to that station. I am assuming that you know which station you want and know its number. For an introduction to tidyhydat see this vignette. You can also search for stations using the tidyhydat::search_stn_name function.\n\nws &lt;- realtime_ws(\n  station_number = \"08MF005\"\n)\nglimpse(ws)\n\nRows: 18,600\nColumns: 10\n$ STATION_NUMBER &lt;chr&gt; \"08MF005\", \"08MF005\", \"08MF005\", \"08MF005\", \"08MF005\", ‚Ä¶\n$ Date           &lt;dttm&gt; 2023-04-04 00:00:00, 2023-04-04 01:00:00, 2023-04-04 0‚Ä¶\n$ Name_En        &lt;chr&gt; \"Water temperature\", \"Water temperature\", \"Water temper‚Ä¶\n$ Value          &lt;dbl&gt; 5.82, 4.87, 4.94, 4.70, 4.21, 3.97, 3.86, 3.81, 3.66, 3‚Ä¶\n$ Unit           &lt;chr&gt; \"¬∞C\", \"¬∞C\", \"¬∞C\", \"¬∞C\", \"¬∞C\", \"¬∞C\", \"¬∞C\", \"¬∞C\", \"¬∞C\", \"‚Ä¶\n$ Grade          &lt;chr&gt; \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"‚Ä¶\n$ Symbol         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ Approval       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,‚Ä¶\n$ Parameter      &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5‚Ä¶\n$ Code           &lt;chr&gt; \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\nParameter\nName_En\n\n\n\n\n46\nWater level (primary sensor)\n\n\n16\nWater level (secondary sensor, telemetry)\n\n\n11\nWater level (secondary sensor)\n\n\n52\nWater level (tertiary sensor, telemetry)\n\n\n13\nWater level (tertiary sensor)\n\n\n3\nWater level (daily mean)\n\n\n39\nWater level (hourly mean)\n\n\n14\nElevation, natural lake\n\n\n42\nElevation, lake or reservoir rule curve\n\n\n17\nAtmospheric pressure\n\n\n18\nAccumulated precipitation\n\n\n19\nIncremental precipitation\n\n\n47\nDischarge (primary sensor derived)\n\n\n7\nDischarge (secondary sensor derived)\n\n\n10\nDischarge (tertiary sensor derived)\n\n\n6\nDischarge (daily mean)\n\n\n40\nDischarge (hourly mean)\n\n\n8\nDischarge (sensor)\n\n\n50\nSnow depth\n\n\n51\nSnow depth, new snowfall\n\n\n1\nAir temperature\n\n\n5\nWater temperature\n\n\n41\nSecondary water temperature\n\n\n34\nWind direction\n\n\n35\nWind speed\n\n\n2\nBattery voltage\n\n\n20\nBlue-green algae\n\n\n21\nConductance\n\n\n26\nTotal dissolved solids\n\n\n43\nDissolved nitrate\n\n\n22\nDissolved oxygen\n\n\n24\npH\n\n\n25\nTurbidity\n\n\n9\nWater velocity\n\n\n37\nWater velocity, x\n\n\n38\nWater velocity, y\n\n\n23\nOxygen saturation\n\n\n49\nChlorophyll\n\n\n28\nRelative humidity\n\n\n36\nCell end\n\n\n4\nInternal equipment temperature\n\n\n12\nTank pressure\n\n\n\n\n\n\nImmediately you can see that the data returned is different than the data returned by realtime_dd. In particular notice the Name_En, Parameter and Code columns. These columns are used to identify the parameters we are interested in. Turns out that you can access more than just hydrometric data via the web service (more on that later!). But for now let‚Äôs just focus on hydrometric data by supplying 47 to the parameter argument to get discharge. Why did I choose 47? I consulted the param_id internal table which tells me that 47 is the parameter code for discharge. In the margin you can see all the other parameters available.\n\nws_discharge &lt;- realtime_ws(\n  station_number = \"08MF005\",\n  parameter = 47\n)\n\nSo how many months back does this data go?\n\nrange(ws_discharge$Date)\n\n[1] \"2023-04-04 00:00:00 UTC\" \"2023-05-04 23:55:00 UTC\"\n\n\nWait - I told you that this would extend back 18 months. What gives? Well the default data range for realtime_ws is 30 days back from today. You can change this by supplying a start_date and end_date argument.\n\nws_discharge &lt;- realtime_ws(\n  station_number = \"08MF005\",\n  parameter = 47,\n  start_date = Sys.Date() - months(18),\n  end_date = Sys.Date()\n)\n\nrange(ws_discharge$Date)\n\n[1] \"2021-11-04 00:00:00 UTC\" \"2023-05-04 23:55:00 UTC\"\n\n\nNow that‚Äôs much better. From here you can make beautiful plots, tables and summaries of that glorious 18 months of data."
  },
  {
    "objectID": "posts/eccc-webservice/index.html#other-parameters",
    "href": "posts/eccc-webservice/index.html#other-parameters",
    "title": "The return of the web service",
    "section": "Other Parameters",
    "text": "Other Parameters\nI did however promise that I would mention something about the other parameters available. The long table to the right lists all the possible parameters. In the water office, you can see (sort of) which parameters are available for a given station. However it is lots of clicking. I currently don‚Äôt know of an easy way to determine which parameters are available for a given station other than just by checking. So for that I‚Äôd recommend querying a station for a short duration.\n\nother_params &lt;- realtime_ws(\n  station_number = \"08MF005\",\n  start_date = Sys.Date() - days(1),\n)\n\nparam_id[param_id$Parameter %in% unique(other_params$Parameter),]\n\n# A tibble: 3 √ó 7\n  Parameter Code  Unit  Name_En            Name_Fr Description_En Description_Fr\n      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;         \n1        46 HG    m     Water level (prim‚Ä¶ Niveau‚Ä¶ Height, stage‚Ä¶ Hauteur, nive‚Ä¶\n2        47 QR    m3/s  Discharge (primar‚Ä¶ Debit ‚Ä¶ Discharge - f‚Ä¶ D√©bit - √©coul‚Ä¶\n3         5 TW    ¬∞C    Water temperature  Temp√©r‚Ä¶ Temperature, ‚Ä¶ Temp√©rature, ‚Ä¶\n\n\nHere we can see that 08MF005, which is the Fraser River at Hope station, also monitors water temperature which has a parameter code of 5. If we re-query the web service, we see that we can fine tune our call to the web service to only return water temperature.\n\nfraser_temp &lt;- realtime_ws(\n  station_number = \"08MF005\",\n  start_date = Sys.Date() - months(18),\n  parameter = 5\n)"
  },
  {
    "objectID": "posts/eccc-webservice/index.html#why-else-might-i-want-to-use-the-web-service",
    "href": "posts/eccc-webservice/index.html#why-else-might-i-want-to-use-the-web-service",
    "title": "The return of the web service",
    "section": "Why else might I want to use the web service?",
    "text": "Why else might I want to use the web service?\nOne other reason you might consider using the web service is because it can be much faster and more efficient that the datamart. We can construct one call to request all the data rather than iterate through multiple station csvs to get what we want. To illustrate this we can construct a simple function that benchmarks the two approaches. (Yes I know that these aren‚Äôt returning exactly the same thing but for these purposes it is good enough.)\n\ncompare_realtime &lt;- function(station_number) {\n  bench::mark(\n    realtime_ws = realtime_ws(\n      station_number = station_number,\n      parameter = c(46, 47)\n    ),\n    realtime_dd = realtime_dd(\n      station_number = station_number,\n    ),\n    max_iterations = 5,\n    check = FALSE\n  )\n}\n\nLet‚Äôs compare the two functions for a single station:\n\ncompare_realtime(\"08MF005\")\n\n# A tibble: 2 √ó 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 realtime_ws    1.78s    1.78s     0.560    7.83MB     0   \n2 realtime_dd     1.7s     1.7s     0.589  593.99MB     8.84\n\n\nOk so on a single station, the two approaches are similar in speed though you can see that lots more memory is being allocated using realtime_dd. By the time you add more stations to the mix, it becomes clear that the web service is a better faster and more efficient approach.\n\ncompare_realtime(c(\"08MF005\", \"08JC002\", \"02LA004\"))\n\n# A tibble: 2 √ó 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 realtime_ws     2.7s     2.7s     0.370   22.38MB     0   \n2 realtime_dd    4.66s    4.66s     0.215    1.73GB     6.44"
  },
  {
    "objectID": "posts/eccc-webservice/index.html#conclusions",
    "href": "posts/eccc-webservice/index.html#conclusions",
    "title": "The return of the web service",
    "section": "Conclusions",
    "text": "Conclusions\nThe web service functionality in tidyhydat is still new so if you notice any funky behaviour please let me know. You can do that by opening an issue in the tidyhydat github repo. This functionality is a nice new way to access Canadian hydrometric data and I am excited to see how people may use it."
  },
  {
    "objectID": "posts/2019-11-17-what-the-buck/index.html",
    "href": "posts/2019-11-17-what-the-buck/index.html",
    "title": "What the Buck?",
    "section": "",
    "text": "I recently appeared on my pal Morgan Tams‚Äô radio program on Cortes Island Radio. The idea is to appear weekly and talk about a single artist/band for 15 minutes. Not terribly ambitious but I thought it would be fun to explore the spotify API and generate some summaries of who we are talking about. For our first installment, Morgan and I chatted about the legendary Buck Owens. Darn‚ÄîI just lost 10 minutes of my life searching for Buck Owens gifs. Oh well. Here is a still of the man:"
  },
  {
    "objectID": "posts/2019-11-17-what-the-buck/index.html#packages",
    "href": "posts/2019-11-17-what-the-buck/index.html#packages",
    "title": "What the Buck?",
    "section": "Packages",
    "text": "Packages\nThankfully there is the R package ‚Äî spotifyr ‚Äî that makes requesting data from the spotify API very easy. Since spotifyr is on CRAN we can install it like usual.\n\ninstall.packages('spotifyr')\n\nFor this post I am also using the following packages which you will need to install.\n\ninstall.packages('dplyr')\ninstall.packages('usethis')\ninstall.packages('ggplot2')\ninstall.packages('tidyr')\n\nand load:\n\nlibrary(spotifyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(usethis)\nlibrary(ggplot2)\nlibrary(tidyr)\n\nThere is some hocus-pocus to set up your credentials to access the spotify data which does require a paid spotify account. The spotifyr developer provides some nice instructions here and the spotify developer guide provides a few more details. Probably the most important thing to note here is that you want to save your spotify credentials in your .Renviron file. If you‚Äôve never dealt with environment variables in R before, Efficient R programming provides a succinct description. In a nutshell our .Renviron file is a way for us to provide the value of a variable consistently across sessions and outside of a script. I always edit it with the usethis package:\n\nedit_r_environ()\n\nSetting up your credentials as environment variables is a one-time thing. After that, functions in the spotifyr package will just work as they all call get_spotify_access_token() by default. Now that I have all credential sorted out let‚Äôs try and see what we can find out about Buck from the spotify API.\n\n\n\n\nbuck_raw &lt;- get_artist_audio_features('buck owens')\n\nWarning in dplyr::left_join(., track_audio_features, by = \"album_id\"): Each row in `x` is expected to match at most 1 row in `y`.\n‚Ñπ Row 1 of `x` matches multiple rows.\n‚Ñπ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\ndim(buck_raw)\n\n[1] 1983   39\n\n\nThis is lots of information (39 columns just on Buck!). With these types of nested JSON data, dplyr‚Äôs glimpse function provides a nice clean way of looking at the data.\n\nglimpse(buck_raw)\n\nRows: 1,983\nColumns: 39\n$ artist_name                  &lt;chr&gt; \"Buck Owens\", \"Buck Owens\", \"Buck Owens\",‚Ä¶\n$ artist_id                    &lt;chr&gt; \"2FMZn5P3WATd7Il6FgPJNu\", \"2FMZn5P3WATd7I‚Ä¶\n$ album_id                     &lt;chr&gt; \"4Owb7bk0AddcMYLLcQHEML\", \"4Owb7bk0AddcMY‚Ä¶\n$ album_type                   &lt;chr&gt; \"album\", \"album\", \"album\", \"album\", \"albu‚Ä¶\n$ album_images                 &lt;list&gt; [&lt;data.frame[3 x 3]&gt;], [&lt;data.frame[3 x ‚Ä¶\n$ album_release_date           &lt;chr&gt; \"2023-01-20\", \"2023-01-20\", \"2023-01-20\",‚Ä¶\n$ album_release_year           &lt;dbl&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023,‚Ä¶\n$ album_release_date_precision &lt;chr&gt; \"day\", \"day\", \"day\", \"day\", \"day\", \"day\",‚Ä¶\n$ danceability                 &lt;dbl&gt; 0.545, 0.620, 0.489, 0.579, 0.456, 0.567,‚Ä¶\n$ energy                       &lt;dbl&gt; 0.398, 0.478, 0.293, 0.355, 0.540, 0.405,‚Ä¶\n$ key                          &lt;int&gt; 8, 9, 3, 8, 10, 4, 9, 9, 3, 2, 8, 7, 7, 6‚Ä¶\n$ loudness                     &lt;dbl&gt; -11.890, -10.100, -10.480, -11.659, -10.0‚Ä¶\n$ mode                         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ speechiness                  &lt;dbl&gt; 0.0386, 0.0625, 0.0330, 0.0320, 0.0443, 0‚Ä¶\n$ acousticness                 &lt;dbl&gt; 0.675, 0.479, 0.579, 0.646, 0.624, 0.198,‚Ä¶\n$ instrumentalness             &lt;dbl&gt; 0.00e+00, 1.99e-04, 3.69e-05, 0.00e+00, 1‚Ä¶\n$ liveness                     &lt;dbl&gt; 0.0666, 0.0551, 0.1230, 0.2520, 0.1600, 0‚Ä¶\n$ valence                      &lt;dbl&gt; 0.718, 0.885, 0.669, 0.799, 0.662, 0.780,‚Ä¶\n$ tempo                        &lt;dbl&gt; 147.685, 165.843, 144.722, 142.691, 146.9‚Ä¶\n$ track_id                     &lt;chr&gt; \"3NFKion7rG1YWSNHD8NKAP\", \"3bjJcKXVNxXBX3‚Ä¶\n$ analysis_url                 &lt;chr&gt; \"https://api.spotify.com/v1/audio-analysi‚Ä¶\n$ time_signature               &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4,‚Ä¶\n$ artists                      &lt;list&gt; [&lt;data.frame[1 x 6]&gt;], [&lt;data.frame[1 x ‚Ä¶\n$ available_markets            &lt;list&gt; &lt;\"AR\", \"AU\", \"AT\", \"BE\", \"BO\", \"BR\", \"BG‚Ä¶\n$ disc_number                  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ duration_ms                  &lt;int&gt; 147173, 123653, 155960, 148386, 149720, 1‚Ä¶\n$ explicit                     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,‚Ä¶\n$ track_href                   &lt;chr&gt; \"https://api.spotify.com/v1/tracks/3NFKio‚Ä¶\n$ is_local                     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,‚Ä¶\n$ track_name                   &lt;chr&gt; \"Above and Beyond\", \"Tired of Livin'\", \"I‚Ä¶\n$ track_preview_url            &lt;chr&gt; \"https://p.scdn.co/mp3-preview/172f00b020‚Ä¶\n$ track_number                 &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13‚Ä¶\n$ type                         &lt;chr&gt; \"track\", \"track\", \"track\", \"track\", \"trac‚Ä¶\n$ track_uri                    &lt;chr&gt; \"spotify:track:3NFKion7rG1YWSNHD8NKAP\", \"‚Ä¶\n$ external_urls.spotify        &lt;chr&gt; \"https://open.spotify.com/track/3NFKion7r‚Ä¶\n$ album_name                   &lt;chr&gt; \"Above and Beyond\", \"Above and Beyond\", \"‚Ä¶\n$ key_name                     &lt;chr&gt; \"G#\", \"A\", \"D#\", \"G#\", \"A#\", \"E\", \"A\", \"A‚Ä¶\n$ mode_name                    &lt;chr&gt; \"major\", \"major\", \"major\", \"major\", \"majo‚Ä¶\n$ key_mode                     &lt;chr&gt; \"G# major\", \"A major\", \"D# major\", \"G# ma‚Ä¶\n\n\nThis is too many columns for now. Let‚Äôs narrow our focus to make it easier to work with.\n\nbuck &lt;- buck_raw %&gt;% \n  select(album_release_date, album_release_year, danceability:tempo, time_signature, \n         duration_ms, track_name, album_name, key_mode)"
  },
  {
    "objectID": "posts/2019-11-17-what-the-buck/index.html#summaries-of-buck",
    "href": "posts/2019-11-17-what-the-buck/index.html#summaries-of-buck",
    "title": "What the Buck?",
    "section": "Summaries of Buck",
    "text": "Summaries of Buck\nWith this data in hand I‚Äôll make some rapid fire summaries of Buck Owens. These summaries turned out not to be particularly compelling radio material but I‚Äôm not going to let that deter me. Taking cue from the spotifyr package, what is Buck‚Äôs most common key?\n\nbuck %&gt;% \n    count(key_mode, sort = TRUE)\n\n   key_mode   n\n1  G# major 326\n2   A major 308\n3  D# major 213\n4   G major 187\n5   E major 165\n6  A# major 137\n7   C major 119\n8   F major 108\n9  F# major 108\n10  D major 101\n11 C# major  94\n12  B major  81\n13  B minor   5\n14 C# minor   5\n15  F minor   5\n16  D minor   4\n17  E minor   4\n18 A# minor   3\n19 F# minor   3\n20  G minor   3\n21 G# minor   3\n22 D# minor   1\n\n\nThe man loved G#/Ab major. It is a bit of unusual key and you can readily find some speculation online about why Buck might have tuned down a half step. And not much in the minor keys. I guess country finds sadness another way. How about time signature?\n\nbuck %&gt;% \n    count(time_signature, sort = TRUE)\n\n  time_signature    n\n1              4 1747\n2              3  214\n3              5   18\n4              1    3\n5              0    1\n\n\nA few suspect data points (zero time signature?) but overall Buck made a career of keep things pretty straight forward. Mostly 4/4 with the occasional waltz.\nWhat about Buck‚Äôs album output. Let‚Äôs plot his cumulative albums over time:\n\ncumulative_albums &lt;- buck %&gt;% \n  select(album_release_year, album_name) %&gt;% \n  distinct(.keep_all = TRUE) %&gt;% \n  count(album_release_year) %&gt;% \n  arrange(album_release_year) %&gt;% \n  mutate(albums = cumsum(n))\n\nggplot(cumulative_albums, aes(x = album_release_year, y = albums)) +\n  geom_line()\n\n\n\n\n\n\n\n\nOk so this data isn‚Äôt particularly good. Likely what would be help is an original_release_date column. Buck was most active in the sixties while the data suggests his output was highest during the mid-nineties. Presumably these are re-issue dates. Still good to know ‚Äî can‚Äôt rely on that year data.\nThe audio features available through the spotify api are very interesting numeric summaries of songs and will be fun to play around with. I won‚Äôt go into descriptions of each audio feature but we will calculate histograms of all Buck‚Äôs songs for each feature. Most features range between 0 and 1 so the distributions can give us a sense of Buck‚Äôs music tendencies.\n\nbuck %&gt;% \n  select(danceability:tempo, duration_ms) %&gt;% \n  gather() %&gt;% \n  mutate(key = tools::toTitleCase(key)) %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = value), fill = \"blue\") +\n  facet_wrap(~key, scales = \"free\")\n\n\n\n\n\n\n\n\nI really like looking at these distributions. Quite what they represent (or how they are derived) is something that I haven‚Äôt quite wrapped my brain around. However they do offer us some high level assessment of an artist‚Äôs catalogue. If the album release date info was better we could do some interesting retrospectives. In another post I‚Äôll try to find a better example. Buck‚Äôs songs are reasonably dancy, don‚Äôt vary much in length and are very positive. This conflicts with my prior of country music being sad and is also likely an interesting hypothesis to further test in a future post.\nLastly let‚Äôs have a look and see if danceability is related to tempo.\n\ncor.test(buck$danceability, buck$tempo, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  buck$danceability and buck$tempo\nS = 1529672927, p-value = 2.015e-15\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.1770145 \n\nbuck %&gt;% \n  filter(danceability != 0) %&gt;% \n  ggplot(aes(x = tempo, y = danceability)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere appears to be a very slight and negative relationship with danceability and tempo. If you are really dancing, you probably want that song to be short. We all only have so much stamina.\nThis has been a short usecase of using the spotify API and in particular the spotifyr package. It is actually pretty exciting to have so much music info at your fingertips."
  },
  {
    "objectID": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html",
    "href": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html",
    "title": "A short history of me at rstudio::conf 2020",
    "section": "",
    "text": "In January, I was lucky enough to attend the 2020 edition of RStudio::conf. Perhaps predictably, the conference and workshops were exceptional and to see all the wonderful things that folks in the R community are capable of was quite inspiring. People are really quite clever. Attending the tidy dev day was such a nice epilogue to the conference because after spending so much time listening to people talk about their code, I was pretty keen to crack open R and have at it myself. Before I lose everything from the conference to memory leaks, I am going to try to catalogue a few things that I learned at the conference by trying to weave them together into a single workflow."
  },
  {
    "objectID": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#a-brief-detour-about-where-to-get-these-packages",
    "href": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#a-brief-detour-about-where-to-get-these-packages",
    "title": "A short history of me at rstudio::conf 2020",
    "section": "A brief detour about where to get these packages",
    "text": "A brief detour about where to get these packages\nSeveral of the packages that I am using here are at development stages and aren‚Äôt yet on CRAN. I‚Äôm including the installation instructions here but eventually this process should be as easy as the typical install.packages. For ggtext, which isn‚Äôt on CRAN, we install it (and the dev version of ggplot2) from GitHub:\n\nremotes::install_github('wilkelab/ggtext')\n\nOther packages that I am using are loaded here:\n\nlibrary(fs)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(arrow)\nlibrary(ggtext)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(here)\nlibrary(stringr)\nlibrary(glue)"
  },
  {
    "objectID": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#the-arrow-package",
    "href": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#the-arrow-package",
    "title": "A short history of me at rstudio::conf 2020",
    "section": "The arrow package",
    "text": "The arrow package\nOne of my main goals at the conference was to find out new ways of dealing with very big flat files. I work in an environment where big flat files are sort of our only option. Adding them to a proper database is not currently possible but I was hopeful that maybe the Apache Arrow project might offer up some solutions. I was not disappointed. Neal Richardson with UrsaLabs gave a great presentation on the status of the project with a specific focus on the R package arrow.\nHere I am mostly parroting what Neal did with his presentation just replacing taxi data with Canadian hydrometric data. Whether we are provisioned data that way or create it ourselves, consider data organized in a hierarchical folder structure. Here at the top level we have Canadian province:\n\n\n/Users/samalbers/_dev/gh_repos/samalbers.science/data/rivers-data\n‚îú‚îÄ‚îÄ AB\n‚îú‚îÄ‚îÄ BC\n‚îú‚îÄ‚îÄ NL\n‚îú‚îÄ‚îÄ SK\n‚îî‚îÄ‚îÄ YT\n\n\nwithin each province folder we have year:\n\n\n/Users/samalbers/_dev/gh_repos/samalbers.science/data/rivers-data/AB\n‚îî‚îÄ‚îÄ 2017\n\n\nwithin each year folder we have month\n\n\n/Users/samalbers/_dev/gh_repos/samalbers.science/data/rivers-data/AB/2017\n‚îú‚îÄ‚îÄ 01\n‚îú‚îÄ‚îÄ 02\n‚îú‚îÄ‚îÄ 03\n‚îú‚îÄ‚îÄ 04\n‚îú‚îÄ‚îÄ 05\n‚îú‚îÄ‚îÄ 06\n‚îú‚îÄ‚îÄ 07\n‚îú‚îÄ‚îÄ 08\n‚îú‚îÄ‚îÄ 09\n‚îú‚îÄ‚îÄ 10\n‚îú‚îÄ‚îÄ 11\n‚îî‚îÄ‚îÄ 12\n\n\nand finally within that directory you actually have your data file:\n\n\n/Users/samalbers/_dev/gh_repos/samalbers.science/data/rivers-data/AB/2017/01\n‚îî‚îÄ‚îÄ rivers.parquet\n\n\nNormally in this situation my approach would be to do some sort of iterative process over each file (mind you still making use of arrow to read the parquet file):\n\ndf_rivs &lt;- list.files(here('data/rivers-data/'), pattern = '*.parquet', recursive = TRUE, full.names = TRUE) %&gt;% \n  map_dfr(read_parquet)\n\nFrom there we might execute some typical sequence designed to filter our data down to a more manageable size.\n\ndf_rivs %&gt;% \n  filter(year(Date) == 2017) %&gt;% \n  filter(Parameter == 'Flow') %&gt;% \n  arrange(Date)\n\n# A tibble: 309,314 √ó 5\n   STATION_NUMBER Date       Parameter  Value Symbol\n   &lt;chr&gt;          &lt;date&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; \n 1 05AA008        2017-01-01 Flow       1.75  B     \n 2 05AA024        2017-01-01 Flow       8.67  &lt;NA&gt;  \n 3 05AA035        2017-01-01 Flow       1.56  B     \n 4 05AC003        2017-01-01 Flow       0.954 B     \n 5 05AC012        2017-01-01 Flow       0.736 B     \n 6 05AC941        2017-01-01 Flow       1.03  &lt;NA&gt;  \n 7 05AD003        2017-01-01 Flow       3.77  B     \n 8 05AD007        2017-01-01 Flow      24.6   B     \n 9 05AE027        2017-01-01 Flow       4.81  B     \n10 05AG006        2017-01-01 Flow      24.7   B     \n# ‚Ä¶ with 309,304 more rows\n\n\nWhat we learned in Neal‚Äôs presentation was the magic of the open_dataset function and specifically its ability to map hierarchical directory structure to virtual columns in your data. If we read just one parquet file, it is apparent that there aren‚Äôt any province, year or month columns:\n\nread_parquet(here('data/rivers-data/AB/2017/01/rivers.parquet'))\n\n# A tibble: 3,131 √ó 5\n   STATION_NUMBER Date       Parameter  Value Symbol\n * &lt;chr&gt;          &lt;date&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; \n 1 05AA008        2017-01-01 Flow       1.75  B     \n 2 05AA024        2017-01-01 Flow       8.67  &lt;NA&gt;  \n 3 05AA035        2017-01-01 Flow       1.56  B     \n 4 05AC003        2017-01-01 Flow       0.954 B     \n 5 05AC012        2017-01-01 Flow       0.736 B     \n 6 05AC941        2017-01-01 Flow       1.03  &lt;NA&gt;  \n 7 05AD003        2017-01-01 Flow       3.77  B     \n 8 05AD007        2017-01-01 Flow      24.6   B     \n 9 05AE027        2017-01-01 Flow       4.81  B     \n10 05AG006        2017-01-01 Flow      24.7   B     \n# ‚Ä¶ with 3,121 more rows\n\n\nInstead, if we assign partitions, using a vector based on the directory structure, open_dataset can use that information to efficiently subset a larger dataset.\n\nrivs &lt;- open_dataset(here('data/rivers-data/'), partitioning = c('province','year', 'month'))\nrivs\n\nFileSystemDataset with 60 Parquet files\nSTATION_NUMBER: string\nDate: date32[day]\nParameter: string\nValue: double\nSymbol: string\nprovince: string\nyear: int32\nmonth: int32\n\nSee $metadata for additional Schema metadata\n\n\nBest of all, the select, filter, group_by and rename dplyr verbs are implemented much like dbplyr and your query is executed lazily taking advantage of both the directory structure and the parquet files.\n\nriver_data &lt;- rivs %&gt;% \n  filter(year == 2017) %&gt;% \n  filter(province %in% c('BC', 'YT', 'AB', 'SK', 'NL')) %&gt;% \n  filter(Parameter == 'Flow') %&gt;% \n  group_by(STATION_NUMBER, province) %&gt;% \n  collect() %&gt;% \n  arrange(Date)\n\nWhile YMMV depending on the size of your data, using folder structure is a nifty way to only access the data we actually need. The Apache Arrow project, and for R users the arrow package, are proceeding very nicely. Now that we have efficiently pared down our river flow data, the next exciting thing I want to explore is some really cool developments in the ggplot2 sphere."
  },
  {
    "objectID": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#the-ggtext-package",
    "href": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#the-ggtext-package",
    "title": "A short history of me at rstudio::conf 2020",
    "section": "The ggtext package",
    "text": "The ggtext package\nThe ggtext package by Claus Wilke provides much improved rendering support for ggplot2. What feels like should a simple task (e.g.¬†colour some portion of text) is sometimes quite onerous in ggplot2. Though the package is still in its infancy, ggtext is breaking trail on making these steps much easier by providing a mini markdown engine directly inside ggplot2. After audible ohhs and ahhs from the crowd while demoing ggtext, Claus observed ‚ÄúI can see this fills a need‚Äù. Already it provides some support for markdown and html rendering.\n\n\n\nSo how can we use it to better visualize our river data? Because ggtext has some minimal html rendering, we can actually include images right inside the ggplot call. My idea was to try and see if I could include provincial flags as axes labels in the plot. This requires steps to:\n\nget the files\nextract the province name from the file name\ncreate a new column in our rivers data because the provinces aren‚Äôt labelled in the same way\nglue the image names with the html snippets\n\nI won‚Äôt go into too much detail but here are the steps:\n\nGet the Data\n\ndir.create(\"data/flags\")\n\ndownload.file(\"https://cdn.britannica.com/40/5440-004-BE91E74F/Flag-Alberta.jpg\", destfile = \"data/flags/AB.jpeg\")\ndownload.file(\"https://cdn.britannica.com/63/5263-004-1C2B7CDE/Flag-Yukon-Territory.jpg\", destfile = \"data/flags/YT.jpeg\")\ndownload.file(\"https://cdn.britannica.com/18/3918-004-9D01BB0E/Flag-Saskatchewan.jpg\", destfile = \"data/flags/SK.jpeg\")\ndownload.file(\"https://cdn.britannica.com/92/2992-004-54C721CF/Flag-Newfoundland-and-Labrador.jpg\", destfile = \"data/flags/NL.jpeg\")\ndownload.file(\"https://cdn.britannica.com/77/6877-004-26251B48/Flag-British-Columbia.jpg\", destfile = \"data/flags/BC.jpeg\")\n\n\n\nExtract Province\n\nflag_paths &lt;- dir_ls(here('data/flags'), glob = '*.jpeg')\n\n\n\nCreate the Image Tags\n\nimg_tags &lt;- glue(\"&lt;img src='{flag_paths}' width='100' /&gt;\")\nnames(img_tags) &lt;- basename(path_ext_remove(flag_paths))\n\nWe now have a named vector, which sources the appropriate provincial flag and is ready to render. This is accomplished by supplying the img_tags vector to the scale of your choice (here scale_x_discrete). ggplot2 knows how to actually render via ggtext::element_markdown. Otherwise we can simply treat this like any other ggplot. Here we are also calculating the annual sum of flows by each value of STATION_NUMBER.\n\nannual_flow &lt;- river_data %&gt;% \n  group_by(STATION_NUMBER, province) %&gt;%\n  summarise(annual_flow = sum(Value, na.rm = TRUE))\n\nannual_flow %&gt;% \n  ggplot(aes(x = province, y = annual_flow)) +\n  geom_point() +\n  scale_x_discrete(name = NULL, labels = img_tags) +\n  theme(axis.text.x = element_markdown(color = 'black', size = 11))\n\n\n\n\n\n\n\n\nSo I think this is pretty cool! We have images in the axes. AFAIK, this was previously impossible. Unfortunately this isn‚Äôt a particularly informative nor a nice looking plot. The next step in the workflow is to change that."
  },
  {
    "objectID": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#design-nicer-plots",
    "href": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#design-nicer-plots",
    "title": "A short history of me at rstudio::conf 2020",
    "section": "Design Nicer Plots",
    "text": "Design Nicer Plots\nRStudio::conf had a thread of design thinking running through the workshops and conference. From the tidyverse‚Äôs near obsession with the design of their api to the inclusion of a live episode of Not So Standard deviation as a keynote, thinking about data science from a design perspective was a key theme that emerged for me. One example of this was Will Chase‚Äôs wonderful talk on the Glamour of Graphics. Will presented some very thoughtful approaches to creating better visualizations. I am going to butcher apply some of those approaches to our plot above.\n\nHydrologically Relevant\nFirst off, our plot is rather uninformative from hydrological perspective. A reasonable goal for this plot would be to aid the user to evaluate the distribution of annual river flow by province. In the above plot, the extreme values are stretching the scale too far out so let‚Äôs limit our analysis to rivers that output less that 10,000 m3/s per year.\n\nannual_flow_sub &lt;- annual_flow %&gt;%\n  filter(annual_flow &lt; 10000)\n\nAlso the basic point plot doesn‚Äôt give us a great way to look at the distribution. For that task, another of Claus‚Äôs packages, ggridges comes in handy. ggridges is great for visualizing distributions and also forces us to flip the axes creating a more natural information flow (at least for those of us that read left to right).\n\nlibrary(ggridges)\n\nannual_flow_sub %&gt;%\n  ggplot(aes(y = province, x = annual_flow)) +\n  geom_density_ridges() +\n  scale_y_discrete(name = NULL, labels = img_tags) +\n  theme(axis.text.y = element_markdown(color = 'black', size = 11))\n\n\n\n\n\n\n\n\nA great line from Will‚Äôs presentation pertained to white space:\n\nWhite space is like garlic; take the amount you think you need, then triple it.\n\nRight then let‚Äôs create some more white space by getting rid of the classic ggplot2 grey background. Here we can also tweak the height of the ridges to better show the distributions.\n\nannual_flow_sub %&gt;%\n  ggplot(aes(y = province, x = annual_flow)) +\n  geom_density_ridges(scale = 1) +\n  scale_y_discrete(name = NULL, labels = img_tags) +\n  theme_minimal() +\n  theme(axis.text.y = element_markdown(color = 'black', size = 11))\n\n\n\n\n\n\n\n\nOk looking a bit better. Another one of Will‚Äôs suggestion is to remove grid lines as much as possible. I basically agree and just keep the minor x values.\n\nannual_flow_sub %&gt;%\n  ggplot(aes(y = province, x = annual_flow)) +\n  geom_density_ridges(scale = 1) +\n  scale_y_discrete(name = NULL, labels = img_tags) +\n  theme_minimal() +\n  theme(axis.text.y = element_markdown(color = 'black', size = 11),\n        panel.grid.major = element_blank(),\n        panel.grid.minor.y = element_blank())\n\n\n\n\n\n\n\n\n‚úîÔ∏è Now we need some colour here. As Will stated colour is hard. My goal here is pretty modest. I just want to distinguish between provinces. To do that I am actually going to steal some colour from the flags and manually map those to fill in colour for the ridges. At the same time I am going to add some transparency to the ridges. I am going to deviate a little from Will‚Äôs advice here and keep the legend. I often get this way with plots and err on the side of caution. In this case I am thinking that folks won‚Äôt recognize the flags and therefore will use the legend. In general though I do like the approach of forcing legends to justify their existence - they need to earn their keep.\n\nflag_cols &lt;- c('#3853a4',\n               '#f3ec18',\n               '#da1a33',\n               '#006b35',\n               '#0054a5')\n\nannual_flow_sub %&gt;%\n  ggplot(aes(y = province, x = annual_flow, fill = province)) +\n  geom_density_ridges(scale = 1, alpha = 0.5) +\n  scale_fill_manual(values = flag_cols) +\n  scale_y_discrete(name = NULL, labels = img_tags) +\n  theme_minimal() +\n  theme(axis.text.y = element_markdown(color = 'black', size = 11),\n        panel.grid.major = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        legend.position = 'bottom',\n        legend.justification='right')\n\n\n\n\n\n\n\n\nLastly this plot needs a title, which according to Will‚Äôs sage advice is also a great way to remove axes labels - just explain it in the title.\n\nannual_flow_sub %&gt;%\n  ggplot(aes(y = province, x = annual_flow, fill = province)) +\n  geom_density_ridges(scale = 1, alpha = 0.5) +\n  scale_fill_manual(name = NULL, values = flag_cols) +\n  scale_y_discrete(name = NULL, labels = img_tags) +\n  labs(title = 'Smoothed distribution of annual flow of gauged rivers (m^3^ s^-1^) by province') +\n  theme_minimal() +\n  theme(axis.text.y = element_markdown(color = 'black', size = 11),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        plot.title.position = 'plot',\n        plot.title = element_markdown(size = 15), \n        legend.position = 'bottom',\n        legend.justification='right')\n\n\n\n\n\n\n\n\nFrom a visualization perspective this isn‚Äôt perfect or even great. Legends are still problematic, I don‚Äôt even know if the flags add anything and grid lines feel like an addiction. Still I think this does provide a decent overview of the types of rivers that are gauged in each province. Remembering that we previously subset all our river flow data to less than 10000, the Yukon gauges bigger rivers while Saskatchewan gauges many smaller rivers. BC and Newfoundland gauge a wide range of rivers types. Alberta gauges rivers that reflect both its mountain and prairies landscapes."
  },
  {
    "objectID": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#back-to-rstudio-conf",
    "href": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#back-to-rstudio-conf",
    "title": "A short history of me at rstudio::conf 2020",
    "section": "Back to RStudio conf",
    "text": "Back to RStudio conf\nThis has been a mini tour through some concept and packages I took in while attending RStudio::conf 2020. I can‚Äôt wait to spend more time with these packages as they mature and development. Every time I connect with the R community, I am grateful to be part of it. RStudio itself presents with class and respect all while creating a positive and inclusive space. I‚Äôm looking forward to future opportunities to connect with all you nerds!"
  },
  {
    "objectID": "posts/hockey-age-with-ojs/index.html",
    "href": "posts/hockey-age-with-ojs/index.html",
    "title": "Talking (Hockey) Age with ObservableJS",
    "section": "",
    "text": "Lately I‚Äôve been thinking about two things: getting old and ObservableJS. Getting old is self-explanatory. ObservableJS (ojs) is maybe less so. Normally ojs is used in a JavaScript notebook environment that enables you to create amazing interactive visualizations. This is a great platform and I suggest that if you are really interested in learning about ojs, you create an account there and start practicing. But sometimes, you want to take your interactive visualizations with you. This is where quarto comes in. Quarto is the successor to rmarkdown and is what this blog is written in. Like rmarkdown you can write prose and code in the same document. The biggest improvement with quarto is that you can also (more) easily write code chunks in other language like python or ojs.\nThis post is about ojs and quarto using some data sourced from the National Hockey League api. So back to talking about being old ‚Äì I was wondering what happens to NHL players when they get old? In particular what happens to their productivity? I was not able to find any interactive visualizations that satisfactorily provided any ability to explore this question. So then I ended up having to learn about the NHL api and how I could get what I wanted out of it. So this is also a post about wrangling data from the NHL api. Almost accidentally, this really highlights the power of quarto. I can much more readily wrangle data in R. Sure it is possible to do this solely in ojs but for me it is easier in R. But then when I want to make the visualization, I can seamlessly switch to ojs right in the same quarto doc.\nLet‚Äôs start with how we get the data. We start with a pretty standard suite of tidyverse packages (and httr2).\nlibrary(httr2)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(glue)\nlibrary(tidyr)\nNext we need to define some helper functions.\nconvert_time &lt;- function(time_str) {\n  # Split the time into minutes and seconds\n  time_parts &lt;- strsplit(time_str, \":\")[[1]]\n  # Convert to integers\n  minutes &lt;- as.integer(time_parts[1])\n  seconds &lt;- as.integer(time_parts[2])\n  # Convert the time to seconds\n  minutes * 60 + seconds\n}\n\ngenerate_year_string &lt;- function(start, end) {\n  # Extract the starting and ending years\n  startYear &lt;- as.numeric(substr(start, 1, 4))\n  endYear &lt;- as.numeric(substr(end, 1, 4))\n  # Generate the sequence of years\n  years &lt;- seq(startYear, endYear)\n  # Concatenate each year with the following one to get the year pairs\n  paste0(years, years + 1)\n}\nconvert_time turns a time string like 824:13 into the number of seconds.\nconvert_time(\"824:13\")\n\n[1] 49453\ngenerate_year_string is a helper function to generate the year strings that the NHL api expects.\ngenerate_year_string(19951996, 19981999)\n\n[1] \"19951996\" \"19961997\" \"19971998\" \"19981999\"\nThis is a function to help sending messages to the console. Defining it here saves lots of space where I use it later on.\napi_message &lt;- function(..., verbose = TRUE) {\n  if (verbose) message(...)\n  invisible(TRUE)\n}\nquery_endpoint is the core function that actually sends a data request to the NHL api. It takes an endpoint and a list of query parameters and returns a tibble of the results. It is always helpful to isolate the code that does the querying from the code that does the data wrangling.\nquery_endpoint &lt;- function(endpoint, ...) {\n  base_url &lt;- \"https://statsapi.web.nhl.com/\"\n  req &lt;- httr2::request(base_url) %&gt;%\n    httr2::req_url_path_append(endpoint) %&gt;%\n    httr2::req_url_query(...)\n\n  resp &lt;- httr2::req_perform(req)\n\n  parsed &lt;- httr2::resp_body_json(resp, simplifyVector = TRUE)\n  parsed$copyright &lt;- NULL\n  dplyr::as_tibble(parsed[[1]])\n}\nWith every request to the NHL api, the following note is returned in the response, specifying exactly who owns all the trademarks and logos. We drop it from the response because it is not really useful for our data wrangling example but I am replicating it here:\nThe get_points_by_age is the workhorse function for this post but is also one of those functions that does just what you want it to do but it may not be broadly useful. For example, this function requests data for every player that is currently on an NHL roster. That‚Äôs a lot of data and the service may get cranky with you for requesting that much. The code is annotated (another cool feature of quarto) to outline the steps of this function. Click on the step and it will highlight the line of code it is referencing.\nget_points_by_age &lt;- function(team, min_games_played, verbose = FALSE) {\n  api_message(glue(\"Getting data for {team}.\"), verbose = verbose)\n\n1  team_df &lt;- query_endpoint(\"api/v1/teams\") %&gt;%\n    filter(teamName == team)\n\n2  team_plus_roster &lt;- query_endpoint(team_df$link, expand = \"team.roster\")\n\n  ## Drop goalies\n  roster &lt;- team_plus_roster[[\"roster\"]][[\"roster\"]][[1]]\n3  person_link &lt;- roster[roster$position != \"G\", ][[\"person\"]]$link\n  person_link &lt;- person_link[!is.na(person_link)]\n\n4  roster_with_ages &lt;- map_df(seq_along(person_link), ~ {\n    query_endpoint(person_link[.x]) %&gt;%\n      select(id, fullName, link, birthDate, currentAge) %&gt;%\n      mutate(birthDate = as.Date(birthDate))\n  })\n\n  point_per_60_by_season &lt;- map_df(seq_along(roster_with_ages$link), ~ {\n    query_url &lt;- glue(\"{roster_with_ages$link[.x]}/stats\")\n\n    season_stats &lt;- map_df(generate_year_string(\"19951996\", \"20222023\"), \\(season) {\n5      stats &lt;- query_endpoint(\n        query_url,\n        stats = \"statsSingleSeason\",\n        season = season\n      )\n\n      ## when there are no stats, return an empty tibble\n      if (length(stats[[\"splits\"]][[1]]) == 0) {\n        api_message(\n          glue(\"No data for {roster_with_ages$fullName[.x]} for the {season} season\", verbose = verbose)\n        )\n        return(tibble())\n      }\n      api_message(\n        glue(\"Getting data for {roster_with_ages$fullName[.x]} for the {season} season\", verbose = verbose)\n      )\n\n6      games_played_threshold &lt;- stats[[\"splits\"]][[1]][[\"stat\"]] %&gt;%\n        filter(games &gt; min_games_played)\n\n      ## when they are not above the threshold, return an empty tibble\n      if (nrow(games_played_threshold) == 0) {\n        api_message(glue(\"{roster_with_ages$fullName[.x]} did not play at least {min_games_played} games in the {season} season\", verbose = verbose))\n        return(tibble())\n      }\n\n      games_played_threshold %&gt;%\n        mutate(evenTimeOnIce = convert_time(evenTimeOnIce)) %&gt;%\n        mutate(even_strength_points = points - powerPlayPoints) %&gt;%\n7        mutate(even_strength_points_per_60 = (even_strength_points / evenTimeOnIce) * 3600) %&gt;%\n        select(even_strength_points_per_60, shotPct) %&gt;%\n        mutate(link = roster_with_ages$link[.x]) %&gt;%\n        mutate(season = season) %&gt;%\n        select(link, season, even_strength_points_per_60, shotPct)\n    })\n    season_stats\n  })\n\n  if (length(point_per_60_by_season) == 0) {\n    api_message(glue(\"No players over found for {team}.\", verbose = verbose))\n    return(tibble())\n  } else {\n    point_per_60_by_season %&gt;%\n      mutate(team = team) %&gt;%\n      left_join(roster_with_ages, by = \"link\") %&gt;%\n8      mutate(age_at_start_of_season = currentAge - (2022 - as.numeric(substr(season, 1, 4)))) %&gt;%\n      filter(!is.na(fullName)) %&gt;%\n      relocate(fullName, birthDate, season, team, .before = even_strength_points_per_60) %&gt;%\n9      as_tibble()\n  }\n}\n\n\n1\n\nGet the team id from the team name\n\n2\n\nGet the roster for that team\n\n3\n\nFilter out the goalies\n\n4\n\nQuery for the age of each player on the roster\n\n5\n\nGet the stats for each player for each season when they actually played\n\n6\n\nFilter out the players that don‚Äôt meet the minimum games played threshold\n\n7\n\nCalculate the points per 60 minutes for each player\n\n8\n\nCalculate the age of the player at the start of the season\n\n9\n\nReturn a tibble with the results"
  },
  {
    "objectID": "posts/hockey-age-with-ojs/index.html#use-the-helper-functions-to-get-the-data",
    "href": "posts/hockey-age-with-ojs/index.html#use-the-helper-functions-to-get-the-data",
    "title": "Talking (Hockey) Age with ObservableJS",
    "section": "Use the helper functions to get the data",
    "text": "Use the helper functions to get the data\nThe way that I‚Äôve written get_points_by_age, you supply it a team name and it will look for stats on those players that are currently on the roster. You also are able to specify a threshold number of games played by individual players. So you would invoke it like this:\n\nget_points_by_age(\"Penguins\", min_games_played = 60, verbose = FALSE)\n\nHowever, since we are interested in all players in the NHL, we need all teams. So first we can submit a query to the teams endpoint to get all the teams .\n\nteams &lt;- query_endpoint(\"api/v1/teams\")\nteams\n\n# A tibble: 32 √ó 15\n      id name                link  venue$name abbreviation teamName locationName\n   &lt;int&gt; &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;       \n 1     1 New Jersey Devils   /api‚Ä¶ Prudentia‚Ä¶ NJD          Devils   New Jersey  \n 2     2 New York Islanders  /api‚Ä¶ UBS Arena  NYI          Islande‚Ä¶ New York    \n 3     3 New York Rangers    /api‚Ä¶ Madison S‚Ä¶ NYR          Rangers  New York    \n 4     4 Philadelphia Flyers /api‚Ä¶ Wells Far‚Ä¶ PHI          Flyers   Philadelphia\n 5     5 Pittsburgh Penguins /api‚Ä¶ PPG Paint‚Ä¶ PIT          Penguins Pittsburgh  \n 6     6 Boston Bruins       /api‚Ä¶ TD Garden  BOS          Bruins   Boston      \n 7     7 Buffalo Sabres      /api‚Ä¶ KeyBank C‚Ä¶ BUF          Sabres   Buffalo     \n 8     8 Montr√©al Canadiens  /api‚Ä¶ Bell Cent‚Ä¶ MTL          Canadie‚Ä¶ Montr√©al    \n 9     9 Ottawa Senators     /api‚Ä¶ Canadian ‚Ä¶ OTT          Senators Ottawa      \n10    10 Toronto Maple Leafs /api‚Ä¶ Scotiaban‚Ä¶ TOR          Maple L‚Ä¶ Toronto     \n# ‚Ñπ 22 more rows\n# ‚Ñπ 12 more variables: venue$link &lt;chr&gt;, $city &lt;chr&gt;, $timeZone &lt;df[,3]&gt;,\n#   $id &lt;int&gt;, firstYearOfPlay &lt;chr&gt;, division &lt;df[,5]&gt;, conference &lt;df[,3]&gt;,\n#   franchise &lt;df[,3]&gt;, shortName &lt;chr&gt;, officialSiteUrl &lt;chr&gt;,\n#   franchiseId &lt;int&gt;, active &lt;lgl&gt;\n\n\nThen we use purrr::map_df to iterate over each team and then bind the results together into a single tibble. This is the data we will be working with.\n\nage_curve_df &lt;- map_df(\n  teams$teamName, ~{\n    get_points_by_age(.x, min_games_played = 40, verbose = interactive())\n  })"
  },
  {
    "objectID": "posts/hockey-age-with-ojs/index.html#the-pass-off-to-ojs",
    "href": "posts/hockey-age-with-ojs/index.html#the-pass-off-to-ojs",
    "title": "Talking (Hockey) Age with ObservableJS",
    "section": "The pass off to ojs",
    "text": "The pass off to ojs\nNow that we have the data, we can pass it off to JavaScript. We do this by using the ojs_define function. This function is available in R and python and it allows you to make data available to any ojs chunk in your quarto document. The age_curve_df data we gathered in the previous steps will now be available for any ojs chunk as ojs_age_curve_df regardless of where it is in the document.\n\nojs_define(ojs_age_curve_df = age_curve_df)"
  },
  {
    "objectID": "posts/hockey-age-with-ojs/index.html#working-with-observable-javascript",
    "href": "posts/hockey-age-with-ojs/index.html#working-with-observable-javascript",
    "title": "Talking (Hockey) Age with ObservableJS",
    "section": "Working with Observable JavaScript",
    "text": "Working with Observable JavaScript\nThe rest of the code in this post is all written in ojs.\nWe do need import libraries (just like R) that don‚Äôt automatically come bundled with quarto. In this example, we are importing the arquero data wrangling library which as far as I can tell, is more or less dplyr for Observable JavaScript. Here is how they describe it:\n\ninspired by the design of dplyr, Arquero provides a fluent API for manipulating column-oriented data frames.\n\n\nimport { aq, op } from '@uwdata/arquero';\n\n\n\n\n\n\nCool! I know I said that I was going to do most of my data wrangling in R but it really does help to be able to do some of it in JavaScript.\nThe first function that we are using is aq.from which is the equivalent of dplyr::as_tibble. It takes a data frame and converts it to an arquero table. We then call the view method on the table to see what it looks like. We are also need to transpose our data frame because arquero expects the data to be in a row-oriented format.\n\nageCurve = aq.from(transpose(ojs_age_curve_df))\nageCurve.view()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe question we are interested in looking at was at what age do players start to decline? Our evaluation metric will be even strength points per 60 minutes. Say, we are interested in looking at playing time for players that are 35 and older. We can filter the ageCurve table using syntax that is very similar to dplyr. The only thing to note is what goes inside the aq.escape function. This is because we are using a JavaScript function inside of a JavaScript function. This escapes the inner function so that it is evaluated properly.\n\nageCurve\n  .filter(aq.escape(d =&gt; d.currentAge &gt; 35))\n\n\n\n\n\n\nOk but you might asking yourself, why don‚Äôt I just do that in dplyr? The beauty of these ojs chunks is that we let the user perform these operations on the fly using interactive tools like sliders, dropdown menus and radio buttons. For that we make use of ObservableJS Inputs. Let‚Äôs make two of these ‚Äì a slider and a dropdown. The slider (via Inputs.range) will define the variable age_cutoff and we can use that value to filter the ageCurve table updating the filteredAgeCurve data as a user interacts with the slider. We then take the filteredAgeCurve to determine unique player names and provide those as the values in the dropdown menu (via Inputs.select). One consequence of that is the values in the dropdown menu are dependent on the slider. Check for yourself. There are many fewer players to highlight at the age cutoff of 35.\nIt doesn‚Äôt get too exciting though until we start to visualize it. I am using the code-fold: true chunk option to hide the code that generates the plot and the inputs so that you can see the plot, the slider and the dropdown more closely together. Have a look at the code comments for a better idea of what is going on.\n\n\nCode\nviewof age_cutoff = Inputs.range([18, 40], {step: 1, value: 30})\n\n// capture the value of the slider and filter the ageCurve table\nfilteredAgeCurve = ageCurve\n  .filter(aq.escape(d =&gt; d.currentAge &gt; age_cutoff)) \n\n// get the unique player names alphabetically\norderedNames = filteredAgeCurve\n  .dedupe(\"fullName\")\n  .orderby(\"fullName\")\n  .array(\"fullName\")\n\n// setup the dropdown\nviewof player = Inputs.select(orderedNames, {value: \"Sidney Crosby\"})\n\n// filter the filteredAgeCurve table to the selected player\nsinglePlayer = filteredAgeCurve\n  .filter(aq.escape(d =&gt; d.fullName == player))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplotMarksHelper = function(data, color) {\n  return Plot.lineY(data, {\n    x: \"age_at_start_of_season\", \n    y: \"even_strength_points_per_60\", \n    z: \"fullName\",\n    stroke: color,\n    curve: \"basis\"\n  });\n}\n\n// setup the plot\nPlot.plot({\n  y: {label: \"Even Strength Points per 60\"},\n  x: {\n    grid: true,\n    label: \"Age at the start of the season\"\n  },\n  marks: [\n    // plot all lines above a certain age\n    // that filteredAgeCurve data changes as the slider changes\n    plotMarksHelper(filteredAgeCurve, \"lightgray\"), \n    // highlight the selected player in blue\n    plotMarksHelper(singlePlayer, \"blue\") \n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are using here the Plot library which also comes bundled with ojs in quarto. I find the code to create plots in ojs pretty accessible. Allison Horst wrote a really nice transition guide from ggplot2 to Plot that has proven to be very helpful. Plot draws from the same grammar of graphics that ggplot2 does so it‚Äôs spirit should feel familiar. Visual properties, like colour, are mapped to variables in the data\nI have not quite found the best way to write nice (read: not smelly) code in ojs but the results are undeniably useful. The plots are beautiful and interactive. Plot provides so many different opportunities for interactivity than other interactive plotting libraries like plotly or ggiraph. Those are great libraries but creating visualization from scratch in ojs feel more natural and flexible."
  },
  {
    "objectID": "posts/hockey-age-with-ojs/index.html#conclusion",
    "href": "posts/hockey-age-with-ojs/index.html#conclusion",
    "title": "Talking (Hockey) Age with ObservableJS",
    "section": "Conclusion",
    "text": "Conclusion\nI would be remiss to just have a pretty plot and not say anything about the data. The slider and the player selector both provide a simple way to explore the data. A couple points to note:\n\nThe overall trend is that many players start to decline at around 30. However there is some clear selection bias here. Players that are not producing at a high level are not going to be playing at 35.\nSidney Crosby is a beast. He is still producing at a consistently high level at 35.\nThe players that you would expect display truly stunning numbers. Nathan Mckinnon is just getting better and better and at 26, there is nothing to suggest that he is slowing down. Connor McDavid entered the league at a stunning pace and has consistency kept it up.\nJoel Pavelski is a complete of an outlier. He produced his best season at 38.\nCorey Perry is also an outlier but in the opposite direction. He has been in a strong decline since he was 30. And yet all he does is compete for Cups so clearly he‚Äôs still doing something right.\n\nTo wrap this up, I think that ojs is a great way to create interactive visualizations. I‚Äôve tried to highlight the handoff from R to ojs and illustrate how you might go about creating a plot.\n\nPhoto by Jeremy Bishop on Unsplash"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sam | science",
    "section": "",
    "text": "I Scream, You Scream, We All Scream for an Arrow Schema\n\n\n\n\n\n\nR\n\n\nApache Arrow\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nSam Albers\n\n\n\n\n\n\n\n\n\n\n\n\nTalking (Hockey) Age with ObservableJS\n\n\n\n\n\n\nR\n\n\nJavaScript\n\n\nObservableJS\n\n\nnhl\n\n\napi\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nSam Albers\n\n\n\n\n\n\n\n\n\n\n\n\nThe return of the web service\n\n\n\n\n\n\nR\n\n\nhydrology\n\n\ntidyhydat\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nSam Albers\n\n\n\n\n\n\n\n\n\n\n\n\nA short history of me at rstudio::conf 2020\n\n\n\n\n\n\nR\n\n\nRStudio\n\n\n\n\n\n\n\n\n\nFeb 11, 2020\n\n\nSam Albers\n\n\n\n\n\n\n\n\n\n\n\n\nOne small step for a package, one giant leap for a task view\n\n\n\n\n\n\nhydrology\n\n\ntask-view\n\n\nR\n\n\n\n\n\n\n\n\n\nJan 19, 2020\n\n\nSam Albers\n\n\n\n\n\n\n\n\n\n\n\n\nWhat the Buck?\n\n\n\n\n\n\nmusic\n\n\nR\n\n\n\nBuck talk\n\n\n\n\n\nNov 17, 2019\n\n\nSam Albers\n\n\n\n\n\n\nNo matching items"
  }
]