[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The repository that builds this blog is located at https://github.com/boshek/samalbers.science"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sam | science",
    "section": "",
    "text": "The return of the web service\n\n\n\n\n\n\n\nR\n\n\nhydrology\n\n\ntidyhydat\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nSam Albers\n\n\n\n\n\n\n  \n\n\n\n\nA short history of me at rstudio::conf 2020\n\n\n\n\n\n\n\nR\n\n\nRStudio\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2020\n\n\nSam Albers\n\n\n\n\n\n\n  \n\n\n\n\nOne small step for a package, one giant leap for a task view\n\n\n\n\n\n\n\nhydrology\n\n\ntask-view\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2020\n\n\nSam Albers\n\n\n\n\n\n\n  \n\n\n\n\nWhat the Buck?\n\n\n\n\n\n\n\nmusic\n\n\nR\n\n\n\n\nBuck talk\n\n\n\n\n\n\nNov 17, 2019\n\n\nSam Albers\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2019-11-17-what-the-buck/index.html",
    "href": "posts/2019-11-17-what-the-buck/index.html",
    "title": "What the Buck?",
    "section": "",
    "text": "I recently appeared on my pal Morgan Tams’ radio program on Cortes Island Radio. The idea is to appear weekly and talk about a single artist/band for 15 minutes. Not terribly ambitious but I thought it would be fun to explore the spotify API and generate some summaries of who we are talking about. For our first installment, Morgan and I chatted about the legendary Buck Owens. Darn—I just lost 10 minutes of my life searching for Buck Owens gifs. Oh well. Here is a still of the man:"
  },
  {
    "objectID": "posts/2019-11-17-what-the-buck/index.html#packages",
    "href": "posts/2019-11-17-what-the-buck/index.html#packages",
    "title": "What the Buck?",
    "section": "Packages",
    "text": "Packages\nThankfully there is the R package — spotifyr — that makes requesting data from the spotify API very easy. Since spotifyr is on CRAN we can install it like usual.\n\ninstall.packages('spotifyr')\n\nFor this post I am also using the following packages which you will need to install.\n\ninstall.packages('dplyr')\ninstall.packages('usethis')\ninstall.packages('ggplot2')\ninstall.packages('tidyr')\n\nand load:\n\nlibrary(spotifyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(usethis)\nlibrary(ggplot2)\nlibrary(tidyr)\n\nThere is some hocus-pocus to set up your credentials to access the spotify data which does require a paid spotify account. The spotifyr developer provides some nice instructions here and the spotify developer guide provides a few more details. Probably the most important thing to note here is that you want to save your spotify credentials in your .Renviron file. If you’ve never dealt with environment variables in R before, Efficient R programming provides a succinct description. In a nutshell our .Renviron file is a way for us to provide the value of a variable consistently across sessions and outside of a script. I always edit it with the usethis package:\n\nedit_r_environ()\n\nSetting up your credentials as environment variables is a one-time thing. After that, functions in the spotifyr package will just work as they all call get_spotify_access_token() by default. Now that I have all credential sorted out let’s try and see what we can find out about Buck from the spotify API.\n\n\n\n\nbuck_raw <- get_artist_audio_features('buck owens')\n\nWarning in dplyr::left_join(., track_audio_features, by = \"album_id\"): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\ndim(buck_raw)\n\n[1] 1983   39\n\n\nThis is lots of information (39 columns just on Buck!). With these types of nested JSON data, dplyr’s glimpse function provides a nice clean way of looking at the data.\n\nglimpse(buck_raw)\n\nRows: 1,983\nColumns: 39\n$ artist_name                  <chr> \"Buck Owens\", \"Buck Owens\", \"Buck Owens\",…\n$ artist_id                    <chr> \"2FMZn5P3WATd7Il6FgPJNu\", \"2FMZn5P3WATd7I…\n$ album_id                     <chr> \"4Owb7bk0AddcMYLLcQHEML\", \"4Owb7bk0AddcMY…\n$ album_type                   <chr> \"album\", \"album\", \"album\", \"album\", \"albu…\n$ album_images                 <list> [<data.frame[3 x 3]>], [<data.frame[3 x …\n$ album_release_date           <chr> \"2023-01-20\", \"2023-01-20\", \"2023-01-20\",…\n$ album_release_year           <dbl> 2023, 2023, 2023, 2023, 2023, 2023, 2023,…\n$ album_release_date_precision <chr> \"day\", \"day\", \"day\", \"day\", \"day\", \"day\",…\n$ danceability                 <dbl> 0.545, 0.620, 0.489, 0.579, 0.456, 0.567,…\n$ energy                       <dbl> 0.398, 0.478, 0.293, 0.355, 0.540, 0.405,…\n$ key                          <int> 8, 9, 3, 8, 10, 4, 9, 9, 3, 2, 8, 7, 7, 6…\n$ loudness                     <dbl> -11.890, -10.100, -10.480, -11.659, -10.0…\n$ mode                         <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ speechiness                  <dbl> 0.0386, 0.0625, 0.0330, 0.0320, 0.0443, 0…\n$ acousticness                 <dbl> 0.675, 0.479, 0.579, 0.646, 0.624, 0.198,…\n$ instrumentalness             <dbl> 0.00e+00, 1.99e-04, 3.69e-05, 0.00e+00, 1…\n$ liveness                     <dbl> 0.0666, 0.0551, 0.1230, 0.2520, 0.1600, 0…\n$ valence                      <dbl> 0.718, 0.885, 0.669, 0.799, 0.662, 0.780,…\n$ tempo                        <dbl> 147.685, 165.843, 144.722, 142.691, 146.9…\n$ track_id                     <chr> \"3NFKion7rG1YWSNHD8NKAP\", \"3bjJcKXVNxXBX3…\n$ analysis_url                 <chr> \"https://api.spotify.com/v1/audio-analysi…\n$ time_signature               <int> 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4,…\n$ artists                      <list> [<data.frame[1 x 6]>], [<data.frame[1 x …\n$ available_markets            <list> <\"AR\", \"AU\", \"AT\", \"BE\", \"BO\", \"BR\", \"BG…\n$ disc_number                  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ duration_ms                  <int> 147173, 123653, 155960, 148386, 149720, 1…\n$ explicit                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ track_href                   <chr> \"https://api.spotify.com/v1/tracks/3NFKio…\n$ is_local                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ track_name                   <chr> \"Above and Beyond\", \"Tired of Livin'\", \"I…\n$ track_preview_url            <chr> \"https://p.scdn.co/mp3-preview/172f00b020…\n$ track_number                 <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13…\n$ type                         <chr> \"track\", \"track\", \"track\", \"track\", \"trac…\n$ track_uri                    <chr> \"spotify:track:3NFKion7rG1YWSNHD8NKAP\", \"…\n$ external_urls.spotify        <chr> \"https://open.spotify.com/track/3NFKion7r…\n$ album_name                   <chr> \"Above and Beyond\", \"Above and Beyond\", \"…\n$ key_name                     <chr> \"G#\", \"A\", \"D#\", \"G#\", \"A#\", \"E\", \"A\", \"A…\n$ mode_name                    <chr> \"major\", \"major\", \"major\", \"major\", \"majo…\n$ key_mode                     <chr> \"G# major\", \"A major\", \"D# major\", \"G# ma…\n\n\nThis is too many columns for now. Let’s narrow our focus to make it easier to work with.\n\nbuck <- buck_raw %>% \n  select(album_release_date, album_release_year, danceability:tempo, time_signature, \n         duration_ms, track_name, album_name, key_mode)"
  },
  {
    "objectID": "posts/2019-11-17-what-the-buck/index.html#summaries-of-buck",
    "href": "posts/2019-11-17-what-the-buck/index.html#summaries-of-buck",
    "title": "What the Buck?",
    "section": "Summaries of Buck",
    "text": "Summaries of Buck\nWith this data in hand I’ll make some rapid fire summaries of Buck Owens. These summaries turned out not to be particularly compelling radio material but I’m not going to let that deter me. Taking cue from the spotifyr package, what is Buck’s most common key?\n\nbuck %>% \n    count(key_mode, sort = TRUE)\n\n   key_mode   n\n1  G# major 326\n2   A major 308\n3  D# major 213\n4   G major 187\n5   E major 165\n6  A# major 137\n7   C major 119\n8   F major 108\n9  F# major 108\n10  D major 101\n11 C# major  94\n12  B major  81\n13  B minor   5\n14 C# minor   5\n15  F minor   5\n16  D minor   4\n17  E minor   4\n18 A# minor   3\n19 F# minor   3\n20  G minor   3\n21 G# minor   3\n22 D# minor   1\n\n\nThe man loved G#/Ab major. It is a bit of unusual key and you can readily find some speculation online about why Buck might have tuned down a half step. And not much in the minor keys. I guess country finds sadness another way. How about time signature?\n\nbuck %>% \n    count(time_signature, sort = TRUE)\n\n  time_signature    n\n1              4 1747\n2              3  214\n3              5   18\n4              1    3\n5              0    1\n\n\nA few suspect data points (zero time signature?) but overall Buck made a career of keep things pretty straight forward. Mostly 4/4 with the occasional waltz.\nWhat about Buck’s album output. Let’s plot his cumulative albums over time:\n\ncumulative_albums <- buck %>% \n  select(album_release_year, album_name) %>% \n  distinct(.keep_all = TRUE) %>% \n  count(album_release_year) %>% \n  arrange(album_release_year) %>% \n  mutate(albums = cumsum(n))\n\nggplot(cumulative_albums, aes(x = album_release_year, y = albums)) +\n  geom_line()\n\n\n\n\n\n\n\n\nOk so this data isn’t particularly good. Likely what would be help is an original_release_date column. Buck was most active in the sixties while the data suggests his output was highest during the mid-nineties. Presumably these are re-issue dates. Still good to know — can’t rely on that year data.\nThe audio features available through the spotify api are very interesting numeric summaries of songs and will be fun to play around with. I won’t go into descriptions of each audio feature but we will calculate histograms of all Buck’s songs for each feature. Most features range between 0 and 1 so the distributions can give us a sense of Buck’s music tendencies.\n\nbuck %>% \n  select(danceability:tempo, duration_ms) %>% \n  gather() %>% \n  mutate(key = tools::toTitleCase(key)) %>% \n  ggplot() +\n  geom_histogram(aes(x = value), fill = \"blue\") +\n  facet_wrap(~key, scales = \"free\")\n\n\n\n\n\n\n\n\nI really like looking at these distributions. Quite what they represent (or how they are derived) is something that I haven’t quite wrapped my brain around. However they do offer us some high level assessment of an artist’s catalogue. If the album release date info was better we could do some interesting retrospectives. In another post I’ll try to find a better example. Buck’s songs are reasonably dancy, don’t vary much in length and are very positive. This conflicts with my prior of country music being sad and is also likely an interesting hypothesis to further test in a future post.\nLastly let’s have a look and see if danceability is related to tempo.\n\ncor.test(buck$danceability, buck$tempo, method = \"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  buck$danceability and buck$tempo\nS = 1529672927, p-value = 2.015e-15\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.1770145 \n\nbuck %>% \n  filter(danceability != 0) %>% \n  ggplot(aes(x = tempo, y = danceability)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere appears to be a very slight and negative relationship with danceability and tempo. If you are really dancing, you probably want that song to be short. We all only have so much stamina.\nThis has been a short usecase of using the spotify API and in particular the spotifyr package. It is actually pretty exciting to have so much music info at your fingertips."
  },
  {
    "objectID": "posts/one-small-step-for-a-package-one-giant-leap-for-a-task-view/index.html",
    "href": "posts/one-small-step-for-a-package-one-giant-leap-for-a-task-view/index.html",
    "title": "One small step for a package, one giant leap for a task view",
    "section": "",
    "text": "This is a bit of a grandiose title for a blog post. But it is important to have ambition right? In 2003, after releasing Michigan, Sufjan Stevens announced his intention to release one album as an ode to each of the fifty American states, appropriately named the Fifty States Project. Despite only completing two albums (the other being Illinois) I appreciate his ambition, and desire to be thorough (and also those wacky costumes).\nIt is that thoroughness which resonates with me and what I’d like to apply to the Hydrology CRAN Task View. Myself, along with Sam Zipper and Ilaria Prosdocimi, maintain the Hydrology task view with the express goal of creating a comprehensive and high–quality list of R packages related to the field of Hydrology. We introduced the task view, at a very high level, over at the rOpenSci blog last year. This post is the first in a series where I try some aspect of every package included in the task view. If Stevens’ can declared his Fifty States Project why can’t I take on the Hydrology Task View Project? My first foray into this is not terribly ambitious as I am going to discuss one of my own packages, rsoi, which was recently updated on CRAN with a new version."
  },
  {
    "objectID": "posts/one-small-step-for-a-package-one-giant-leap-for-a-task-view/index.html#rsoi",
    "href": "posts/one-small-step-for-a-package-one-giant-leap-for-a-task-view/index.html#rsoi",
    "title": "One small step for a package, one giant leap for a task view",
    "section": "rsoi",
    "text": "rsoi\nrsoi started out as a package to acquire El Niño-Southern Oscillation (ENSO) data and import it into R. This is the data that climate scientists use to determine which phase of the oscillation we are currently experiencing. Since that initial version, rsoi has gained several datasets and now functions more generally as simple R interface to as many climate indices as we can collect. All the raw data that rsoi accesses is directly from the US government’s National Oceanic and Atmospehric Administration (NOAA). Packages like rsoi facilitate reproducible workflows by providing convenient functions to access data stored on the web. (Sidebar: the most comprehensive package for accessing all manner of NOAA data is the rnoaa package by Scott Chamberlain which is also in the Hydrology task view 😉). As of version 0.5.1 rsoi provides access to these data (with the corresponding rsoi function in parentheses):\n\nSouthern Oscillation Index (download_soi)\nOceanic Nino Index (download_oni)\nNorth Pacific Gyre Oscillation (download_npgo)\nNorth Atlantic Oscillation (download_nao)\nArctic Oscillation (download_ao)\nAntarctic Oscillation (download_aao)\nMultivariate ENSO Index Version 2 (download_mei)\n\nrsoi is on CRAN so installation proceeds typically:\n\ninstall.packages(\"rsoi\")\n\nThe usage of a rsoi revolves around the download_* functions. For example if you want to read in the Oceanic Nino Index data you can run this code:\n\nlibrary(rsoi)\n\noni <- download_oni()\nhead(oni)\n\n# A tibble: 6 × 7\n   Year Month Date       dSST3.4   ONI ONI_month_window phase             \n  <int> <ord> <date>       <dbl> <dbl> <chr>            <fct>             \n1  1950 Jan   1950-01-01   -1.62 NA    <NA>             <NA>              \n2  1950 Feb   1950-02-01   -1.32 -1.34 JFM              Cool Phase/La Nina\n3  1950 Mar   1950-03-01   -1.07 -1.17 FMA              Cool Phase/La Nina\n4  1950 Apr   1950-04-01   -1.11 -1.18 MAM              Cool Phase/La Nina\n5  1950 May   1950-05-01   -1.37 -1.07 AMJ              Cool Phase/La Nina\n6  1950 Jun   1950-06-01   -0.74 -0.85 MJJ              Cool Phase/La Nina\n\n\nEach climate index in rsoi is accessed by functions of the format download_[index abbreviation]. A quick plot of the ONI data demonstrates that we are currently in the Neutral ENSO phase (as of 19/01/2020).\n\n\n\n\n\n\n\n\n\nThis is the basic usage of rsoi that has been in place since it was first published to CRAN. Luckily Elio Campitelli found rsoi and made it better:\n\n\nMe: \"I've been downloading and reshaping ENSO data often. Maybe I should build an #rstats package to do it. \"Also me: \"Mh.. I wonder if someone else has already done it\"The internets: \"Here you go\"https://t.co/CC7hVFXEjY\n\n— Elio Campitelli (@d_olivaw) October 17, 2019\n\n\nAs of version 0.5.1, thanks to several contributions by Elio, rsoi now will optionally use a cache so that data are downloaded only once per R session. Not only this is more polite to NOAA as the data provider, this also provides some optimization for processes that repeateadly call rsoi functions. You can either use this cache in memory:\n\nlibrary(tictoc)\n\n## accessing from the web\ntic()\nsoi <- download_soi(use_cache = TRUE)\ntoc()\n\n1.028 sec elapsed\n\n##same session, accessing from the memory cache\ntic()\nsoi <- download_soi(use_cache = TRUE)\ntoc()\n\n0.006 sec elapsed\n\n\nOr you can save it locally:\n\nmei <- download_mei(use_cache = TRUE, file = \"mei.csv\")\n\nSubsequent calls of download_mei that have the use_cache argument set to TRUE, will automatically import that data stored on disk rather than access it from the web. This works for each function in rsoi. This is a really nice contribution from Elio.\nA knock on CRAN Task Views, meritted or not, that I have heard is that they are simply an unvetted collection of packages. To a certain extent this is true as there is no formal process whereby a package gains acceptance to a task view. Rather package maintainers usually simply ask to be added and they are. This series of blog posts (n currently equalling 1) is designed to provide some additional exploration of packages in the Hydrology task view. Like Sufjan Stevens, this is an ambitious goal. However, writing a blog is much easier than making an album (have you heard Illinois?) so maybe I have a better chance."
  },
  {
    "objectID": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html",
    "href": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html",
    "title": "A short history of me at rstudio::conf 2020",
    "section": "",
    "text": "In January, I was lucky enough to attend the 2020 edition of RStudio::conf. Perhaps predictably, the conference and workshops were exceptional and to see all the wonderful things that folks in the R community are capable of was quite inspiring. People are really quite clever. Attending the tidy dev day was such a nice epilogue to the conference because after spending so much time listening to people talk about their code, I was pretty keen to crack open R and have at it myself. Before I lose everything from the conference to memory leaks, I am going to try to catalogue a few things that I learned at the conference by trying to weave them together into a single workflow."
  },
  {
    "objectID": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#a-brief-detour-about-where-to-get-these-packages",
    "href": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#a-brief-detour-about-where-to-get-these-packages",
    "title": "A short history of me at rstudio::conf 2020",
    "section": "A brief detour about where to get these packages",
    "text": "A brief detour about where to get these packages\nSeveral of the packages that I am using here are at development stages and aren’t yet on CRAN. I’m including the installation instructions here but eventually this process should be as easy as the typical install.packages. For ggtext, which isn’t on CRAN, we install it (and the dev version of ggplot2) from GitHub:\n\nremotes::install_github('wilkelab/ggtext')\n\nOther packages that I am using are loaded here:\n\nlibrary(fs)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(arrow)\nlibrary(ggtext)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(here)\nlibrary(stringr)\nlibrary(glue)"
  },
  {
    "objectID": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#the-arrow-package",
    "href": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#the-arrow-package",
    "title": "A short history of me at rstudio::conf 2020",
    "section": "The arrow package",
    "text": "The arrow package\nOne of my main goals at the conference was to find out new ways of dealing with very big flat files. I work in an environment where big flat files are sort of our only option. Adding them to a proper database is not currently possible but I was hopeful that maybe the Apache Arrow project might offer up some solutions. I was not disappointed. Neal Richardson with UrsaLabs gave a great presentation on the status of the project with a specific focus on the R package arrow.\nHere I am mostly parroting what Neal did with his presentation just replacing taxi data with Canadian hydrometric data. Whether we are provisioned data that way or create it ourselves, consider data organized in a hierarchical folder structure. Here at the top level we have Canadian province:\n\n\n/Users/samalbers/_dev/gh_repos/samalbers.science/data/rivers-data\n├── AB\n├── BC\n├── NL\n├── SK\n└── YT\n\n\nwithin each province folder we have year:\n\n\n/Users/samalbers/_dev/gh_repos/samalbers.science/data/rivers-data/AB\n└── 2017\n\n\nwithin each year folder we have month\n\n\n/Users/samalbers/_dev/gh_repos/samalbers.science/data/rivers-data/AB/2017\n├── 01\n├── 02\n├── 03\n├── 04\n├── 05\n├── 06\n├── 07\n├── 08\n├── 09\n├── 10\n├── 11\n└── 12\n\n\nand finally within that directory you actually have your data file:\n\n\n/Users/samalbers/_dev/gh_repos/samalbers.science/data/rivers-data/AB/2017/01\n└── rivers.parquet\n\n\nNormally in this situation my approach would be to do some sort of iterative process over each file (mind you still making use of arrow to read the parquet file):\n\ndf_rivs <- list.files(here('data/rivers-data/'), pattern = '*.parquet', recursive = TRUE, full.names = TRUE) %>% \n  map_dfr(read_parquet)\n\nFrom there we might execute some typical sequence designed to filter our data down to a more manageable size.\n\ndf_rivs %>% \n  filter(year(Date) == 2017) %>% \n  filter(Parameter == 'Flow') %>% \n  arrange(Date)\n\n# A tibble: 309,314 × 5\n   STATION_NUMBER Date       Parameter  Value Symbol\n   <chr>          <date>     <chr>      <dbl> <chr> \n 1 05AA008        2017-01-01 Flow       1.75  B     \n 2 05AA024        2017-01-01 Flow       8.67  <NA>  \n 3 05AA035        2017-01-01 Flow       1.56  B     \n 4 05AC003        2017-01-01 Flow       0.954 B     \n 5 05AC012        2017-01-01 Flow       0.736 B     \n 6 05AC941        2017-01-01 Flow       1.03  <NA>  \n 7 05AD003        2017-01-01 Flow       3.77  B     \n 8 05AD007        2017-01-01 Flow      24.6   B     \n 9 05AE027        2017-01-01 Flow       4.81  B     \n10 05AG006        2017-01-01 Flow      24.7   B     \n# … with 309,304 more rows\n\n\nWhat we learned in Neal’s presentation was the magic of the open_dataset function and specifically its ability to map hierarchical directory structure to virtual columns in your data. If we read just one parquet file, it is apparent that there aren’t any province, year or month columns:\n\nread_parquet(here('data/rivers-data/AB/2017/01/rivers.parquet'))\n\n# A tibble: 3,131 × 5\n   STATION_NUMBER Date       Parameter  Value Symbol\n * <chr>          <date>     <chr>      <dbl> <chr> \n 1 05AA008        2017-01-01 Flow       1.75  B     \n 2 05AA024        2017-01-01 Flow       8.67  <NA>  \n 3 05AA035        2017-01-01 Flow       1.56  B     \n 4 05AC003        2017-01-01 Flow       0.954 B     \n 5 05AC012        2017-01-01 Flow       0.736 B     \n 6 05AC941        2017-01-01 Flow       1.03  <NA>  \n 7 05AD003        2017-01-01 Flow       3.77  B     \n 8 05AD007        2017-01-01 Flow      24.6   B     \n 9 05AE027        2017-01-01 Flow       4.81  B     \n10 05AG006        2017-01-01 Flow      24.7   B     \n# … with 3,121 more rows\n\n\nInstead, if we assign partitions, using a vector based on the directory structure, open_dataset can use that information to efficiently subset a larger dataset.\n\nrivs <- open_dataset(here('data/rivers-data/'), partitioning = c('province','year', 'month'))\nrivs\n\nFileSystemDataset with 60 Parquet files\nSTATION_NUMBER: string\nDate: date32[day]\nParameter: string\nValue: double\nSymbol: string\nprovince: string\nyear: int32\nmonth: int32\n\nSee $metadata for additional Schema metadata\n\n\nBest of all, the select, filter, group_by and rename dplyr verbs are implemented much like dbplyr and your query is executed lazily taking advantage of both the directory structure and the parquet files.\n\nriver_data <- rivs %>% \n  filter(year == 2017) %>% \n  filter(province %in% c('BC', 'YT', 'AB', 'SK', 'NL')) %>% \n  filter(Parameter == 'Flow') %>% \n  group_by(STATION_NUMBER, province) %>% \n  collect() %>% \n  arrange(Date)\n\nWhile YMMV depending on the size of your data, using folder structure is a nifty way to only access the data we actually need. The Apache Arrow project, and for R users the arrow package, are proceeding very nicely. Now that we have efficiently pared down our river flow data, the next exciting thing I want to explore is some really cool developments in the ggplot2 sphere."
  },
  {
    "objectID": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#the-ggtext-package",
    "href": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#the-ggtext-package",
    "title": "A short history of me at rstudio::conf 2020",
    "section": "The ggtext package",
    "text": "The ggtext package\nThe ggtext package by Claus Wilke provides much improved rendering support for ggplot2. What feels like should a simple task (e.g. colour some portion of text) is sometimes quite onerous in ggplot2. Though the package is still in its infancy, ggtext is breaking trail on making these steps much easier by providing a mini markdown engine directly inside ggplot2. After audible ohhs and ahhs from the crowd while demoing ggtext, Claus observed “I can see this fills a need”. Already it provides some support for markdown and html rendering.\n\n\n\nSo how can we use it to better visualize our river data? Because ggtext has some minimal html rendering, we can actually include images right inside the ggplot call. My idea was to try and see if I could include provincial flags as axes labels in the plot. This requires steps to:\n\nget the files\nextract the province name from the file name\ncreate a new column in our rivers data because the provinces aren’t labelled in the same way\nglue the image names with the html snippets\n\nI won’t go into too much detail but here are the steps:\n\nGet the Data\n\ndir.create(\"data/flags\")\n\ndownload.file(\"https://cdn.britannica.com/40/5440-004-BE91E74F/Flag-Alberta.jpg\", destfile = \"data/flags/AB.jpeg\")\ndownload.file(\"https://cdn.britannica.com/63/5263-004-1C2B7CDE/Flag-Yukon-Territory.jpg\", destfile = \"data/flags/YT.jpeg\")\ndownload.file(\"https://cdn.britannica.com/18/3918-004-9D01BB0E/Flag-Saskatchewan.jpg\", destfile = \"data/flags/SK.jpeg\")\ndownload.file(\"https://cdn.britannica.com/92/2992-004-54C721CF/Flag-Newfoundland-and-Labrador.jpg\", destfile = \"data/flags/NL.jpeg\")\ndownload.file(\"https://cdn.britannica.com/77/6877-004-26251B48/Flag-British-Columbia.jpg\", destfile = \"data/flags/BC.jpeg\")\n\n\n\nExtract Province\n\nflag_paths <- dir_ls(here('data/flags'), glob = '*.jpeg')\n\n\n\nCreate the Image Tags\n\nimg_tags <- glue(\"<img src='{flag_paths}' width='100' />\")\nnames(img_tags) <- basename(path_ext_remove(flag_paths))\n\nWe now have a named vector, which sources the appropriate provincial flag and is ready to render. This is accomplished by supplying the img_tags vector to the scale of your choice (here scale_x_discrete). ggplot2 knows how to actually render via ggtext::element_markdown. Otherwise we can simply treat this like any other ggplot. Here we are also calculating the annual sum of flows by each value of STATION_NUMBER.\n\nannual_flow <- river_data %>% \n  group_by(STATION_NUMBER, province) %>%\n  summarise(annual_flow = sum(Value, na.rm = TRUE))\n\nannual_flow %>% \n  ggplot(aes(x = province, y = annual_flow)) +\n  geom_point() +\n  scale_x_discrete(name = NULL, labels = img_tags) +\n  theme(axis.text.x = element_markdown(color = 'black', size = 11))\n\n\n\n\nSo I think this is pretty cool! We have images in the axes. AFAIK, this was previously impossible. Unfortunately this isn’t a particularly informative nor a nice looking plot. The next step in the workflow is to change that."
  },
  {
    "objectID": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#design-nicer-plots",
    "href": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#design-nicer-plots",
    "title": "A short history of me at rstudio::conf 2020",
    "section": "Design Nicer Plots",
    "text": "Design Nicer Plots\nRStudio::conf had a thread of design thinking running through the workshops and conference. From the tidyverse’s near obsession with the design of their api to the inclusion of a live episode of Not So Standard deviation as a keynote, thinking about data science from a design perspective was a key theme that emerged for me. One example of this was Will Chase’s wonderful talk on the Glamour of Graphics. Will presented some very thoughtful approaches to creating better visualizations. I am going to butcher apply some of those approaches to our plot above.\n\nHydrologically Relevant\nFirst off, our plot is rather uninformative from hydrological perspective. A reasonable goal for this plot would be to aid the user to evaluate the distribution of annual river flow by province. In the above plot, the extreme values are stretching the scale too far out so let’s limit our analysis to rivers that output less that 10,000 m3/s per year.\n\nannual_flow_sub <- annual_flow %>%\n  filter(annual_flow < 10000)\n\nAlso the basic point plot doesn’t give us a great way to look at the distribution. For that task, another of Claus’s packages, ggridges comes in handy. ggridges is great for visualizing distributions and also forces us to flip the axes creating a more natural information flow (at least for those of us that read left to right).\n\nlibrary(ggridges)\n\nannual_flow_sub %>%\n  ggplot(aes(y = province, x = annual_flow)) +\n  geom_density_ridges() +\n  scale_y_discrete(name = NULL, labels = img_tags) +\n  theme(axis.text.y = element_markdown(color = 'black', size = 11))\n\n\n\n\nA great line from Will’s presentation pertained to white space:\n\nWhite space is like garlic; take the amount you think you need, then triple it.\n\nRight then let’s create some more white space by getting rid of the classic ggplot2 grey background. Here we can also tweak the height of the ridges to better show the distributions.\n\nannual_flow_sub %>%\n  ggplot(aes(y = province, x = annual_flow)) +\n  geom_density_ridges(scale = 1) +\n  scale_y_discrete(name = NULL, labels = img_tags) +\n  theme_minimal() +\n  theme(axis.text.y = element_markdown(color = 'black', size = 11))\n\n\n\n\nOk looking a bit better. Another one of Will’s suggestion is to remove grid lines as much as possible. I basically agree and just keep the minor x values.\n\nannual_flow_sub %>%\n  ggplot(aes(y = province, x = annual_flow)) +\n  geom_density_ridges(scale = 1) +\n  scale_y_discrete(name = NULL, labels = img_tags) +\n  theme_minimal() +\n  theme(axis.text.y = element_markdown(color = 'black', size = 11),\n        panel.grid.major = element_blank(),\n        panel.grid.minor.y = element_blank())\n\n\n\n\n✔️ Now we need some colour here. As Will stated colour is hard. My goal here is pretty modest. I just want to distinguish between provinces. To do that I am actually going to steal some colour from the flags and manually map those to fill in colour for the ridges. At the same time I am going to add some transparency to the ridges. I am going to deviate a little from Will’s advice here and keep the legend. I often get this way with plots and err on the side of caution. In this case I am thinking that folks won’t recognize the flags and therefore will use the legend. In general though I do like the approach of forcing legends to justify their existence - they need to earn their keep.\n\nflag_cols <- c('#3853a4',\n               '#f3ec18',\n               '#da1a33',\n               '#006b35',\n               '#0054a5')\n\nannual_flow_sub %>%\n  ggplot(aes(y = province, x = annual_flow, fill = province)) +\n  geom_density_ridges(scale = 1, alpha = 0.5) +\n  scale_fill_manual(values = flag_cols) +\n  scale_y_discrete(name = NULL, labels = img_tags) +\n  theme_minimal() +\n  theme(axis.text.y = element_markdown(color = 'black', size = 11),\n        panel.grid.major = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        legend.position = 'bottom',\n        legend.justification='right')\n\n\n\n\nLastly this plot needs a title, which according to Will’s sage advice is also a great way to remove axes labels - just explain it in the title.\n\nannual_flow_sub %>%\n  ggplot(aes(y = province, x = annual_flow, fill = province)) +\n  geom_density_ridges(scale = 1, alpha = 0.5) +\n  scale_fill_manual(name = NULL, values = flag_cols) +\n  scale_y_discrete(name = NULL, labels = img_tags) +\n  labs(title = 'Smoothed distribution of annual flow of gauged rivers (m^3^ s^-1^) by province') +\n  theme_minimal() +\n  theme(axis.text.y = element_markdown(color = 'black', size = 11),\n        axis.title.x = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        plot.title.position = 'plot',\n        plot.title = element_markdown(size = 15), \n        legend.position = 'bottom',\n        legend.justification='right')\n\n\n\n\nFrom a visualization perspective this isn’t perfect or even great. Legends are still problematic, I don’t even know if the flags add anything and grid lines feel like an addiction. Still I think this does provide a decent overview of the types of rivers that are gauged in each province. Remembering that we previously subset all our river flow data to less than 10000, the Yukon gauges bigger rivers while Saskatchewan gauges many smaller rivers. BC and Newfoundland gauge a wide range of rivers types. Alberta gauges rivers that reflect both its mountain and prairies landscapes."
  },
  {
    "objectID": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#back-to-rstudio-conf",
    "href": "posts/a-short-history-of-me-at-rstudio-conf-2020/index.html#back-to-rstudio-conf",
    "title": "A short history of me at rstudio::conf 2020",
    "section": "Back to RStudio conf",
    "text": "Back to RStudio conf\nThis has been a mini tour through some concept and packages I took in while attending RStudio::conf 2020. I can’t wait to spend more time with these packages as they mature and development. Every time I connect with the R community, I am grateful to be part of it. RStudio itself presents with class and respect all while creating a positive and inclusive space. I’m looking forward to future opportunities to connect with all you nerds!"
  },
  {
    "objectID": "posts/eccc-webservice/index.html",
    "href": "posts/eccc-webservice/index.html",
    "title": "The return of the web service",
    "section": "",
    "text": "The most common question I get about the tidyhydat package goes something like this:\nPreviously the answer was… you can’t. The HYDAT database is a historical database of hydrometric data. Data are validated and entered into HYDAT periodically. It is not updated in realtime. At the same time realtime data is only available for 30 days from the datamart.\nNow, however, Environment and Climate Change Canada (ECCC) provided a web service that provides realtime data for stations which extends back to about 18 months. This usually spans the gap for current data to when it gets into HYDAT. And since tidyhydat version 0.6.0 you can now access this data in R via the realtime_ws function. This post is a quick introduction to some of the usage of the web service from tidyhydat.\nLet’s load a few packages to help illustrate this.\nlibrary(tidyhydat)\nlibrary(dplyr)\nlibrary(lubridate)"
  },
  {
    "objectID": "posts/eccc-webservice/index.html#new-parameters",
    "href": "posts/eccc-webservice/index.html#new-parameters",
    "title": "The return of the webservice",
    "section": "New Parameters",
    "text": "New Parameters\nSo once you chosen your measure you can see if your station has data for that measure. So a good first step is to grab all the stations that are also reporting in realtime\n\nstations <- realtime_stations()\nstations\n\n# A tibble: 2,061 × 6\n   STATION_NUMBER STATION_NAME   LATITUDE LONGITUDE PROV_TERR_STATE_LOC TIMEZONE\n   <chr>          <chr>             <dbl>     <dbl> <chr>               <chr>   \n 1 01AD003        ST. FRANCIS R…     47.2     -69.0 NB                  UTC-04:…\n 2 01AD004        SAINT JOHN RI…     47.4     -68.3 NB                  UTC-04:…\n 3 01AF002        SAINT JOHN RI…     47.0     -67.7 NB                  UTC-04:…\n 4 01AF007        GRANDE RIVIER…     47.2     -67.9 NB                  UTC-04:…\n 5 01AF009        IROQUOIS RIVE…     47.5     -68.4 NB                  UTC-04:…\n 6 01AG003        AROOSTOOK RIV…     46.8     -67.8 NB                  UTC-04:…\n 7 01AH002        TOBIQUE RIVER…     47.2     -67.2 NB                  UTC-04:…\n 8 01AJ003        MEDUXNEKEAG R…     46.2     -67.7 NB                  UTC-04:…\n 9 01AJ004        BIG PRESQUE I…     46.4     -67.7 NB                  UTC-04:…\n10 01AJ010        BECAGUIMEC ST…     46.3     -67.5 NB                  UTC-04:…\n# ℹ 2,051 more rows\n\n\nOk so 2061 stations are reporting in realtime. There is no direct way to see"
  },
  {
    "objectID": "posts/eccc-webservice/index.html#getting-realtime-hydrometric-data",
    "href": "posts/eccc-webservice/index.html#getting-realtime-hydrometric-data",
    "title": "The return of the webservice",
    "section": "Getting realtime hydrometric data",
    "text": "Getting realtime hydrometric data\nThe realtime_ws function operates in a similar way to the realtime_dd function. You can get data for a single station or for a list of stations and the function returns a tibble. Here I am assuming that you know which station you and know its number. For an introduction to tidyhydat see this vignette.\n\nws &lt;- realtime_ws(\n  station_number = \"02JE032\"\n)\nglimpse(ws)\n\nRows: 8,928\nColumns: 10\n$ STATION_NUMBER &lt;chr&gt; \"02JE032\", \"02JE032\", \"02JE032\", \"02JE032\", \"02JE032\", …\n$ Date           &lt;dttm&gt; 2023-04-03 00:00:00, 2023-04-03 00:05:00, 2023-04-03 0…\n$ Name_En        &lt;chr&gt; \"Water level (primary sensor)\", \"Water level (primary s…\n$ Value          &lt;dbl&gt; 6.484, 6.482, 6.477, 6.475, 6.476, 6.473, 6.473, 6.480,…\n$ Unit           &lt;chr&gt; \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", …\n$ Grade          &lt;chr&gt; \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"…\n$ Symbol         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Approval       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Parameter      &lt;dbl&gt; 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46,…\n$ Code           &lt;chr&gt; \"HG\", \"HG\", \"HG\", \"HG\", \"HG\", \"HG\", \"HG\", \"HG\", \"HG\", \"…\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Parameter\n      Name_En\n    \n  \n  \n    46\nWater level (primary sensor)\n    16\nWater level (secondary sensor, telemetry)\n    11\nWater level (secondary sensor)\n    52\nWater level (tertiary sensor, telemetry)\n    13\nWater level (tertiary sensor)\n    3\nWater level (daily mean)\n    39\nWater level (hourly mean)\n    14\nElevation, natural lake\n    42\nElevation, lake or reservoir rule curve\n    17\nAtmospheric pressure\n    18\nAccumulated precipitation\n    19\nIncremental precipitation\n    47\nDischarge (primary sensor derived)\n    7\nDischarge (secondary sensor derived)\n    10\nDischarge (tertiary sensor derived)\n    6\nDischarge (daily mean)\n    40\nDischarge (hourly mean)\n    8\nDischarge (sensor)\n    50\nSnow depth\n    51\nSnow depth, new snowfall\n    1\nAir temperature\n    5\nWater temperature\n    41\nSecondary water temperature\n    34\nWind direction\n    35\nWind speed\n    2\nBattery voltage\n    20\nBlue-green algae\n    21\nConductance\n    26\nTotal dissolved solids\n    43\nDissolved nitrate\n    22\nDissolved oxygen\n    24\npH\n    25\nTurbidity\n    9\nWater velocity\n    37\nWater velocity, x\n    38\nWater velocity, y\n    23\nOxygen saturation\n    49\nChlorophyll\n    28\nRelative humidity\n    36\nCell end\n    4\nInternal equipment temperature\n    12\nTank pressure\n  \n  \n  \n\n\n\nImmediately you can see that the data returned is different than the data returned by realtime_dd. In particular notice the Name_En, Parameter and Code columns. These columns are used to identify the parameters we are interested in. Turns out off that you can access more than just hydrometric data via the webservice (more on that later!). But for now let’s just focus on hydrometric data by supplying 47 to the parameter argument to get discharge. Why did I choose 47? I used the param_id internal table which tells me that 47 is the parameter code for discharge. In the margin you can see all the other parameters available.\n\nws_discharge &lt;- realtime_ws(\n  station_number = \"02JE032\",\n  parameter = 46\n)\n\nSo how many months back does this data go?\n\nrange(ws_discharge$Date)\n\n[1] \"2023-04-03 00:00:00 UTC\" \"2023-05-03 23:55:00 UTC\"\n\n\nWait - I told you that this would extend back 18 months. What gives? Well the default data range for realtime_ws is 30 days. You can change this by supplying a start_date and end_date argument.\n\nws_discharge &lt;- realtime_ws(\n  station_number = \"02JE032\",\n  parameter = c(46, 5),\n  start_date = Sys.Date() - months(18),\n  end_date = Sys.Date()\n)\n\nrange(ws_discharge$Date)\n\n[1] \"2021-11-03 00:00:00 UTC\" \"2023-05-03 23:55:00 UTC\"\n\n\nNow that’s much better. From here you can make beautiful plots, tables and summaries of that glorious 18 months of data."
  },
  {
    "objectID": "posts/eccc-webservice/index.html#other-parameters",
    "href": "posts/eccc-webservice/index.html#other-parameters",
    "title": "The return of the web service",
    "section": "Other Parameters",
    "text": "Other Parameters\nI did however promise that I would mention something about the other parameters available. The long table to the right lists all the possible parameters. In the water office, you can see (sort of) which parameters are available for a given station. However it is lots of clicking. I currently don’t know of an easy way to determine which parameters are available for a given station other than just by checking. So for that I’d recommend querying a station for a short duration.\n\nother_params &lt;- realtime_ws(\n  station_number = \"08MF005\",\n  start_date = Sys.Date() - days(1),\n)\n\nparam_id[param_id$Parameter %in% unique(other_params$Parameter),]\n\n# A tibble: 3 × 7\n  Parameter Code  Unit  Name_En            Name_Fr Description_En Description_Fr\n      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;         \n1        46 HG    m     Water level (prim… Niveau… Height, stage… Hauteur, nive…\n2        47 QR    m3/s  Discharge (primar… Debit … Discharge - f… Débit - écoul…\n3         5 TW    °C    Water temperature  Tempér… Temperature, … Température, …\n\n\nHere we can see that 08MF005, which is the Fraser River at Hope station, also monitors water temperature which has a parameter code of 5. If we re-query the web service, we see that we can fine tune our call to the web service to only return water temperature.\n\nfraser_temp &lt;- realtime_ws(\n  station_number = \"08MF005\",\n  start_date = Sys.Date() - months(18),\n  parameter = 5\n)"
  },
  {
    "objectID": "posts/eccc-webservice/index.html#using-the-webservice-for-realtime-hydrometric-data",
    "href": "posts/eccc-webservice/index.html#using-the-webservice-for-realtime-hydrometric-data",
    "title": "The return of the webservice",
    "section": "Using the webservice for realtime hydrometric data",
    "text": "Using the webservice for realtime hydrometric data\nThe realtime_ws function operates in a similar way to most of the other functions in tidyhydat particularly the realtime_dd function. You can pass a single station or a vector of stations and the function returns a tibble of that data. Here I am assuming that you know which station you want and know its number. For an introduction to tidyhydat see this vignette.\n\nws &lt;- realtime_ws(\n  station_number = \"08MF005\"\n)\nglimpse(ws)\n\nRows: 18,600\nColumns: 10\n$ STATION_NUMBER &lt;chr&gt; \"08MF005\", \"08MF005\", \"08MF005\", \"08MF005\", \"08MF005\", …\n$ Date           &lt;dttm&gt; 2023-04-04 00:00:00, 2023-04-04 01:00:00, 2023-04-04 0…\n$ Name_En        &lt;chr&gt; \"Water temperature\", \"Water temperature\", \"Water temper…\n$ Value          &lt;dbl&gt; 5.82, 4.87, 4.94, 4.70, 4.21, 3.97, 3.86, 3.81, 3.66, 3…\n$ Unit           &lt;chr&gt; \"°C\", \"°C\", \"°C\", \"°C\", \"°C\", \"°C\", \"°C\", \"°C\", \"°C\", \"…\n$ Grade          &lt;chr&gt; \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"…\n$ Symbol         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Approval       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Parameter      &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5…\n$ Code           &lt;chr&gt; \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"…\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Parameter\n      Name_En\n    \n  \n  \n    46\nWater level (primary sensor)\n    16\nWater level (secondary sensor, telemetry)\n    11\nWater level (secondary sensor)\n    52\nWater level (tertiary sensor, telemetry)\n    13\nWater level (tertiary sensor)\n    3\nWater level (daily mean)\n    39\nWater level (hourly mean)\n    14\nElevation, natural lake\n    42\nElevation, lake or reservoir rule curve\n    17\nAtmospheric pressure\n    18\nAccumulated precipitation\n    19\nIncremental precipitation\n    47\nDischarge (primary sensor derived)\n    7\nDischarge (secondary sensor derived)\n    10\nDischarge (tertiary sensor derived)\n    6\nDischarge (daily mean)\n    40\nDischarge (hourly mean)\n    8\nDischarge (sensor)\n    50\nSnow depth\n    51\nSnow depth, new snowfall\n    1\nAir temperature\n    5\nWater temperature\n    41\nSecondary water temperature\n    34\nWind direction\n    35\nWind speed\n    2\nBattery voltage\n    20\nBlue-green algae\n    21\nConductance\n    26\nTotal dissolved solids\n    43\nDissolved nitrate\n    22\nDissolved oxygen\n    24\npH\n    25\nTurbidity\n    9\nWater velocity\n    37\nWater velocity, x\n    38\nWater velocity, y\n    23\nOxygen saturation\n    49\nChlorophyll\n    28\nRelative humidity\n    36\nCell end\n    4\nInternal equipment temperature\n    12\nTank pressure\n  \n  \n  \n\n\n\nImmediately you can see that the data returned is different than the data returned by realtime_dd. In particular notice the Name_En, Parameter and Code columns. These columns are used to identify the parameters we are interested in. Turns out off that you can access more than just hydrometric data via the webservice (more on that later!). But for now let’s just focus on hydrometric data by supplying 47 to the parameter argument to get discharge. Why did I choose 47? I consulted the param_id internal table which tells me that 47 is the parameter code for discharge. In the margin you can see all the other parameters available.\n\nws_discharge &lt;- realtime_ws(\n  station_number = \"08MF005\",\n  parameter = 47\n)\n\nSo how many months back does this data go?\n\nrange(ws_discharge$Date)\n\n[1] \"2023-04-04 00:00:00 UTC\" \"2023-05-04 23:55:00 UTC\"\n\n\nWait - I told you that this would extend back 18 months. What gives? Well the default data range for realtime_ws is 30 days back from today. You can change this by supplying a start_date and end_date argument.\n\nws_discharge &lt;- realtime_ws(\n  station_number = \"08MF005\",\n  parameter = 47,\n  start_date = Sys.Date() - months(18),\n  end_date = Sys.Date()\n)\n\nrange(ws_discharge$Date)\n\n[1] \"2021-11-04 00:00:00 UTC\" \"2023-05-04 23:55:00 UTC\"\n\n\nNow that’s much better. From here you can make beautiful plots, tables and summaries of that glorious 18 months of data."
  },
  {
    "objectID": "posts/eccc-webservice/index.html#why-else-might-i-want-to-use-the-webservice",
    "href": "posts/eccc-webservice/index.html#why-else-might-i-want-to-use-the-webservice",
    "title": "The return of the webservice",
    "section": "Why else might I want to use the webservice?",
    "text": "Why else might I want to use the webservice?\nOne other reason you might consider using the webservice is because it can be much faster and more efficient that the datamart. We can construct one call to request all the data rather than iterate through multiple station csvs to get what we want. To illustrate this we can construct a simple function that benchmarks the two approaches. (Yes I know that these aren’t returning exactly the same thing but for these purposes it is good enough.)\n\ncompare_realtime &lt;- function(station_number) {\n  bench::mark(\n    realtime_ws = realtime_ws(\n      station_number = station_number,\n      parameter = c(46, 47)\n    ),\n    realtime_dd = realtime_dd(\n      station_number = station_number,\n    ),\n    max_iterations = 5,\n    check = FALSE\n  )\n}\n\nLet’s compare the two functions for a single station:\n\ncompare_realtime(\"08MF005\")\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 realtime_ws     1.8s     1.8s     0.555    7.83MB     0   \n2 realtime_dd    1.61s    1.61s     0.620  593.18MB     9.29\n\n\nOk so on a single station, the two approaches are similar in speed though you can see that lots more memory is being allocated using realtime_dd. Let’s try a few more stations:\n\ncompare_realtime(c(\"08MF005\", \"08JC002\", \"02LA004\"))\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 realtime_ws    2.76s    2.76s     0.362   22.38MB    0.362\n2 realtime_dd    4.45s    4.45s     0.225    1.73GB    6.97"
  },
  {
    "objectID": "posts/eccc-webservice/index.html#using-the-web-service-for-realtime-hydrometric-data",
    "href": "posts/eccc-webservice/index.html#using-the-web-service-for-realtime-hydrometric-data",
    "title": "The return of the web service",
    "section": "Using the web service for realtime hydrometric data",
    "text": "Using the web service for realtime hydrometric data\nThe realtime_ws function operates in a similar way to most of the other functions in tidyhydat particularly the realtime_dd function. You can pass a single station or a vector of stations and the function returns a tibble of data relating to that station. I am assuming that you know which station you want and know its number. For an introduction to tidyhydat see this vignette. You can also search for stations using the tidyhydat::search_stn_name function.\n\nws &lt;- realtime_ws(\n  station_number = \"08MF005\"\n)\nglimpse(ws)\n\nRows: 18,600\nColumns: 10\n$ STATION_NUMBER &lt;chr&gt; \"08MF005\", \"08MF005\", \"08MF005\", \"08MF005\", \"08MF005\", …\n$ Date           &lt;dttm&gt; 2023-04-04 00:00:00, 2023-04-04 01:00:00, 2023-04-04 0…\n$ Name_En        &lt;chr&gt; \"Water temperature\", \"Water temperature\", \"Water temper…\n$ Value          &lt;dbl&gt; 5.82, 4.87, 4.94, 4.70, 4.21, 3.97, 3.86, 3.81, 3.66, 3…\n$ Unit           &lt;chr&gt; \"°C\", \"°C\", \"°C\", \"°C\", \"°C\", \"°C\", \"°C\", \"°C\", \"°C\", \"…\n$ Grade          &lt;chr&gt; \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"-1\", \"…\n$ Symbol         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Approval       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Parameter      &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5…\n$ Code           &lt;chr&gt; \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"TW\", \"…\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Parameter\n      Name_En\n    \n  \n  \n    46\nWater level (primary sensor)\n    16\nWater level (secondary sensor, telemetry)\n    11\nWater level (secondary sensor)\n    52\nWater level (tertiary sensor, telemetry)\n    13\nWater level (tertiary sensor)\n    3\nWater level (daily mean)\n    39\nWater level (hourly mean)\n    14\nElevation, natural lake\n    42\nElevation, lake or reservoir rule curve\n    17\nAtmospheric pressure\n    18\nAccumulated precipitation\n    19\nIncremental precipitation\n    47\nDischarge (primary sensor derived)\n    7\nDischarge (secondary sensor derived)\n    10\nDischarge (tertiary sensor derived)\n    6\nDischarge (daily mean)\n    40\nDischarge (hourly mean)\n    8\nDischarge (sensor)\n    50\nSnow depth\n    51\nSnow depth, new snowfall\n    1\nAir temperature\n    5\nWater temperature\n    41\nSecondary water temperature\n    34\nWind direction\n    35\nWind speed\n    2\nBattery voltage\n    20\nBlue-green algae\n    21\nConductance\n    26\nTotal dissolved solids\n    43\nDissolved nitrate\n    22\nDissolved oxygen\n    24\npH\n    25\nTurbidity\n    9\nWater velocity\n    37\nWater velocity, x\n    38\nWater velocity, y\n    23\nOxygen saturation\n    49\nChlorophyll\n    28\nRelative humidity\n    36\nCell end\n    4\nInternal equipment temperature\n    12\nTank pressure\n  \n  \n  \n\n\n\nImmediately you can see that the data returned is different than the data returned by realtime_dd. In particular notice the Name_En, Parameter and Code columns. These columns are used to identify the parameters we are interested in. Turns out that you can access more than just hydrometric data via the web service (more on that later!). But for now let’s just focus on hydrometric data by supplying 47 to the parameter argument to get discharge. Why did I choose 47? I consulted the param_id internal table which tells me that 47 is the parameter code for discharge. In the margin you can see all the other parameters available.\n\nws_discharge &lt;- realtime_ws(\n  station_number = \"08MF005\",\n  parameter = 47\n)\n\nSo how many months back does this data go?\n\nrange(ws_discharge$Date)\n\n[1] \"2023-04-04 00:00:00 UTC\" \"2023-05-04 23:55:00 UTC\"\n\n\nWait - I told you that this would extend back 18 months. What gives? Well the default data range for realtime_ws is 30 days back from today. You can change this by supplying a start_date and end_date argument.\n\nws_discharge &lt;- realtime_ws(\n  station_number = \"08MF005\",\n  parameter = 47,\n  start_date = Sys.Date() - months(18),\n  end_date = Sys.Date()\n)\n\nrange(ws_discharge$Date)\n\n[1] \"2021-11-04 00:00:00 UTC\" \"2023-05-04 23:55:00 UTC\"\n\n\nNow that’s much better. From here you can make beautiful plots, tables and summaries of that glorious 18 months of data."
  },
  {
    "objectID": "posts/eccc-webservice/index.html#why-else-might-i-want-to-use-the-web-service",
    "href": "posts/eccc-webservice/index.html#why-else-might-i-want-to-use-the-web-service",
    "title": "The return of the web service",
    "section": "Why else might I want to use the web service?",
    "text": "Why else might I want to use the web service?\nOne other reason you might consider using the web service is because it can be much faster and more efficient that the datamart. We can construct one call to request all the data rather than iterate through multiple station csvs to get what we want. To illustrate this we can construct a simple function that benchmarks the two approaches. (Yes I know that these aren’t returning exactly the same thing but for these purposes it is good enough.)\n\ncompare_realtime &lt;- function(station_number) {\n  bench::mark(\n    realtime_ws = realtime_ws(\n      station_number = station_number,\n      parameter = c(46, 47)\n    ),\n    realtime_dd = realtime_dd(\n      station_number = station_number,\n    ),\n    max_iterations = 5,\n    check = FALSE\n  )\n}\n\nLet’s compare the two functions for a single station:\n\ncompare_realtime(\"08MF005\")\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 realtime_ws     1.8s     1.8s     0.555    7.83MB     0   \n2 realtime_dd    3.25s    3.25s     0.308  593.99MB     4.62\n\n\nOk so on a single station, the two approaches are similar in speed though you can see that lots more memory is being allocated using realtime_dd. By the time you add more stations to the mix, it becomes clear that the web service is a better faster and more efficient approach.\n\ncompare_realtime(c(\"08MF005\", \"08JC002\", \"02LA004\"))\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 realtime_ws    2.68s    2.68s     0.373   22.38MB     0   \n2 realtime_dd    4.56s    4.56s     0.219    1.73GB     6.58"
  },
  {
    "objectID": "posts/eccc-webservice/index.html#conclusions",
    "href": "posts/eccc-webservice/index.html#conclusions",
    "title": "The return of the web service",
    "section": "Conclusions",
    "text": "Conclusions\nThe web service functionality in tidyhydat is still new so if you notice any funky behaviour please let me know. You can do that by opening an issue in the tidyhydat github repo. This functionality is a nice new way to access Canadian hydrometric data and I am excited to see how people may use it."
  }
]