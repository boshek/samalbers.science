{
  "hash": "da7c29ee50ac61b122f2b1f3000bed3d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: I Scream, You Scream, We All Scream for an Arrow Schema\nauthor: Sam Albers\ndate: last-modified\nslug: you-schema\nexecute: \n  warning: true\ncategories:\n  - R\n  - Apache Arrow\n  - python\ntags: []\n\ndescription: ''\nfeatured: ''\nfeaturedalt: ''\nfeaturedpath: ''\nlinktitle: ''\ntype: post\n---\n\n\n\n\n\nAlways take the time to write a schema. Sometimes you have to. It is isn't optional. But in less strongly typed languages like R and python, you can sometimes get away with not specifying a schema because there are such good tools for correctly inferring it. This post is an example of an instance where you absolutely do need to provide schema. \n\nA data structure that is popular is a hive partitioned dataset. This is a cheap data structure that is easy to create (just structured directories!), compatible with s3 and has many tools to take advantage of it. I say \"cheap\" because it does not come with the overhead of a full fledged database but on some level can act like one. For example, you can efficiently query partitioned data with SQL just like a typical database. \n\nTo illustrate why schemas are important for a partitioned dataset we first need to create one. We are going to use R's internal `quakes` data. The code below creates a reprex but the main thing to notice is that one partition, `stations=50`, contains an additional column called `wave_height`. This scenario mimics a situation where for that station, perhaps you gained a new sensor and thus collected more data (always good!). \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow, warn.conflicts = FALSE)\nfor (i in unique(quakes$stations)) {\n  quakes_per_station <- quakes[quakes$stations == i, ]\n  if (i == \"50\") {\n    quakes_per_station$wave_height <- runif(nrow(quakes_per_station), 0, 100)\n  }\n  write_dataset(quakes_per_station, \"quakes_partitioned/\", partitioning = \"stations\")\n}\n```\n:::\n\n\n\nThe directory `quakes_partitioned` will have a bunch of sub-directories of the formation `{variable}={value}`. \n\n```\nquakes_partitioned/\n├── stations=10\n```\n\nIf we examine just one of those files, we can see what it contains:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_parquet(\"quakes_partitioned/stations=10/part-0.parquet\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 × 4\n     lat  long depth   mag\n   <dbl> <dbl> <int> <dbl>\n 1 -21    182.   600   4.4\n 2 -23.6  181.   349   4  \n 3 -16.3  186     48   4.5\n 4 -20.1  184.   186   4.2\n 5 -15.0  182.   399   4.1\n 6 -19.1  169.   158   4.4\n 7 -17.7  185    383   4  \n 8 -21.0  181.   483   4.2\n 9 -27.2  182.    55   4.6\n10 -18.4  183.   343   4.1\n11 -20.3  182.   476   4.5\n12 -14.8  185.   294   4.1\n13 -17.6  182.   548   4.1\n14 -20.6  182.   518   4.2\n15 -25    180    488   4.5\n16 -17.8  185.   223   4.1\n17 -20.7  186.    80   4  \n18 -21.8  181    618   4.1\n19 -21.0  181.   616   4.3\n20 -17.7  188.    45   4.2\n```\n\n\n:::\n:::\n\n\n\nNote that the station column has been dropped from the parquet file because it is contained within the partitioned structure (i.e. the directory name). \n\nApache Arrow has considerable functionality for dealing with partitioned data via its [dataset layer](https://arrow.apache.org/docs/java/dataset.html). The easiest way for us to take advantage of this in R is to use `open_dataset`, point it at the partitioned directory _and_ tell it that there is a partitioned variable:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(\"quakes_partitioned/\", partitioning = \"stations\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFileSystemDataset with 102 Parquet files\n5 columns\nlat: double\nlong: double\ndepth: int32\nmag: double\nstations: int32\n\nSee $metadata for additional Schema metadata\n```\n\n\n:::\n:::\n\n\n\nThis is a very fast operation that quickly collects some basic metadata from our `quakes` dataset. However we can see clearly that we are missing the added `water_temp` column from station 50. If we investigate that single parquet file we can see that column there:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_parquet(\"quakes_partitioned/stations=50/part-0.parquet\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 5\n     lat  long depth   mag wave_height\n   <dbl> <dbl> <int> <dbl>       <dbl>\n 1 -22.6  181.   544   5          66.2\n 2 -20.6  182.   529   5          41.6\n 3 -22.9  173.    56   5.1        75.3\n 4 -23.3  184    164   4.8        79.6\n 5 -20.5  182.   559   4.9        65.7\n 6 -26.5  178.   609   5          28.9\n 7 -25.0  180.   505   4.9        47.1\n 8 -23.4  180.   541   4.6        83.2\n 9 -23.9  180.   524   4.6        43.7\n10 -20.9  185.    82   4.9        68.7\n```\n\n\n:::\n:::\n\n\n\nSo what's happened here? An arrow dataset just uses the first file to construct the metadata. Because `water_temp` is not present there, it is not picked up by a `open_dataset` call. \n\nHow can we fix this? A schema! If we take the time to create a schema, we can tell `open_dataset`: \"hey there is another column in there that we are also interested in\". The arrow R package provides a [`schema` function](https://arrow.apache.org/docs/r/reference/schema.html) to construct this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquake_schema <- schema(\n  lat = float64(),\n  long = float64(),\n  depth = int32(),\n  mag = float64(),\n  wave_height = float64(),\n  station = string()\n)\nquake_schema\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSchema\nlat: double\nlong: double\ndepth: int32\nmag: double\nwave_height: double\nstation: string\n```\n\n\n:::\n:::\n\n\n\nNow we can provide that schema to the `open_dataset` function and see what happens:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquakes_dataset <- open_dataset(\n  \"quakes_partitioned/\",\n  partitioning = \"stations\",\n  schema = quake_schema\n)\nquakes_dataset\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFileSystemDataset with 102 Parquet files\n6 columns\nlat: double\nlong: double\ndepth: int32\nmag: double\nwave_height: double\nstation: string\n```\n\n\n:::\n:::\n\n\n\n`open_dataset` has recognized our `wave_height` column! We explicitly told arrow about it through a schema and so now you can work with it as you would any other column. \n\nOne other option that is worth discussing is setting the `unify_schemas` argument to `TRUE`. From the arrow documentation:\n\n> should all data fragments (files, Datasets) be scanned in order to create a unified schema from them? If FALSE, only the first fragment will be inspected for its schema. Use this fast path when you know and trust that all fragments have an identical schema. The default is FALSE when creating a dataset from a directory path/URI or vector of file paths/URIs (because there may be many files and scanning may be slow) but TRUE when sources is a list of Datasets (because there should be few Datasets in the list and their Schemas are already in memory).\n\nThis is an excellent heuristic for when you should use `unify_schemas`. I'd also suggest that you also gain some benefit from explicitly writing our your schema types as well the fields being used. Also specifying the schema in the way discussed above happens to be a bit a tiny bit faster on these data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bench)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nbench::mark(\n  set_schema = open_dataset(\n    \"quakes_partitioned/\",\n    partitioning = \"stations\",\n    schema = quake_schema\n  ) |>\n    collect(),\n  unify_schema = open_dataset(\n    \"quakes_partitioned/\",\n    partitioning = \"stations\",\n    unify_schemas = TRUE\n  ) |>\n    collect(),\n  check = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  expression        min  median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr>   <bch:tm> <bch:t>     <dbl> <bch:byt>    <dbl>\n1 set_schema     32.9ms  34.9ms      28.6    1.08MB     4.76\n2 unify_schema   36.6ms  38.6ms      25.8   16.38KB     5.17\n```\n\n\n:::\n:::\n\n\n\nIt is also worth mentioning that this is not a R specific thing. The same pattern exists in pyarrow:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pyarrow as pa\nimport pyarrow.dataset as ds\n\nquakes_partitioned = ds.dataset(\"quakes_partitioned/\", partitioning = \"hive\")\nquakes_partitioned.schema\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlat: double\nlong: double\ndepth: int32\nmag: double\nstations: int32\n-- schema metadata --\nr: 'A\n3\n263168\n197888\n5\nUTF-8\n531\n1\n531\n5\n254\n254\n254\n254\n254\n1026\n1\n2621' + 128\n```\n\n\n:::\n:::\n\n\n\nI am not certain quite what the schema metadata is telling us here and that'll have to be an investigation for another day. But the thing you can notice here is that, again, in the absense of a schema, we don't detect that `wave_height` column. But if we instead provide it, we are able to detect it:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nquake_schema = pa.schema(\n    [\n        (\"lat\", pa.float64()),\n        (\"long\", pa.float64()),\n        (\"depth\", pa.int32()),\n        (\"mag\", pa.float64()),\n        (\"wave_height\", pa.float64()),\n        (\"stations\", pa.string()),\n    ]\n)\n\nquakes_partitioned = ds.dataset(\"quakes_partitioned/\", partitioning = \"hive\", schema=quake_schema)\nquakes_partitioned.schema\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlat: double\nlong: double\ndepth: int32\nmag: double\nwave_height: double\nstations: string\n```\n\n\n:::\n:::\n\n\n\nExplicitly setting your schema, illustrated here with Apache Arrow datasets, can be really important. You always have a schema - this process just reminds us that you either let it be inferred by another program or you tell your computer exactly what you want. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}